# Copilot Instructions for Crypto Data Collection System

These instructions are specifically designed for GitHub Copilot to understand the data collection system architecture, development patterns, and best practices.

## ðŸŽ¯ **System Context & Architecture**

### **Core System Identity**
You are working with a **dedicated Kubernetes-based data collection system** that operates as an isolated node in a 4-node cryptocurrency trading infrastructure. This system is responsible for collecting, processing, and serving financial market data through APIs.

### **System Boundaries & Responsibilities**
- **Primary Purpose**: Data collection, processing, and API serving (NOT trading operations)
- **Deployment**: Kubernetes namespace `crypto-collectors` on dedicated node
- **Database**: Connects to Windows MySQL via `host.docker.internal`
- **External APIs**: CoinGecko Premium, FRED, Guardian, Reddit, OpenAI
- **Consumers**: Trading engine node, analytics node, external systems

### **Key Architectural Principles**
1. **Isolation**: No trading logic - pure data collection and serving
2. **API-First**: All data access through well-defined REST/WebSocket/GraphQL APIs
3. **Kubernetes-Native**: All services run as Kubernetes deployments
4. **Microservices**: 14 collectors + 4 processors + 3 API services
5. **External Database**: Uses existing Windows MySQL, not in-cluster databases

## ðŸ”§ **Development Patterns & Conventions**

### **Service Structure Pattern**
When creating new data collectors or processors, follow this structure:

```python
# Standard collector service pattern
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional
import mysql.connector
import aiohttp
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# Configuration using environment variables
import os
DB_CONFIG = {
    'host': os.getenv('MYSQL_HOST', 'host.docker.internal'),
    'user': os.getenv('MYSQL_USER', 'news_collector'), 
    'password': os.getenv('MYSQL_PASSWORD', '99Rules!'),
    'database': os.getenv('MYSQL_DATABASE', 'crypto_prices')
}

class DataCollector:
    def __init__(self):
        self.app = FastAPI(title="Your Collector Service")
        self.setup_routes()
        self.session = None
        
    def setup_routes(self):
        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy", "timestamp": datetime.utcnow()}
            
        @self.app.get("/metrics")
        async def metrics():
            return {"collector": "your_service", "status": "operational"}
    
    async def collect_data(self, symbols: List[str]) -> Dict:
        # Use aiohttp for external API calls
        async with aiohttp.ClientSession() as session:
            # Implementation here
            pass
    
    def store_data(self, data: Dict):
        # Use mysql.connector for database operations
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        try:
            # Database operations
            pass
        finally:
            cursor.close()
            conn.close()

if __name__ == "__main__":
    import uvicorn
    collector = DataCollector()
    uvicorn.run(collector.app, host="0.0.0.0", port=8080)
```

### **Database Connection Pattern**
ALWAYS use this exact configuration for database connections:

```python
import mysql.connector
from mysql.connector import pooling

# Standard database configuration
DB_CONFIG = {
    'host': 'host.docker.internal',  # CRITICAL: Use this for K8s to Windows
    'user': 'news_collector',        # CRITICAL: Use this user
    'password': '99Rules!',          # Standard password
    'database': 'crypto_prices',     # Primary database
    'charset': 'utf8mb4',
    'autocommit': True
}

# Use connection pooling for better performance
connection_pool = pooling.MySQLConnectionPool(
    pool_name="crypto_pool",
    pool_size=10,
    **DB_CONFIG
)

def get_db_connection():
    return connection_pool.get_connection()
```

### **Kubernetes Configuration Pattern**
All services MUST include these Kubernetes patterns:

```yaml
# deployment.yaml template
apiVersion: apps/v1
kind: Deployment
metadata:
  name: your-service-name
  namespace: crypto-collectors
  labels:
    app: data-collection
    component: collector  # or processor, api
    service: your-service-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: your-service-name
  template:
    metadata:
      labels:
        app: your-service-name
    spec:
      nodeSelector:
        node-type: data-collection  # Target data collection node
      containers:
      - name: your-service-name
        image: your-service:latest
        ports:
        - containerPort: 8080
        env:
        - name: MYSQL_HOST
          valueFrom:
            configMapKeyRef:
              name: database-config
              key: MYSQL_HOST
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: api-keys
              key: YOUR_API_KEY
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: your-service-name
  namespace: crypto-collectors
spec:
  selector:
    app: your-service-name
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
```

### **API Design Patterns**
Follow these patterns for consistent API design:

```python
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime

app = FastAPI(
    title="Your Service API",
    description="Data collection service for crypto markets",
    version="1.0.0"
)

# Standard response models
class HealthResponse(BaseModel):
    status: str
    timestamp: datetime
    service: str
    version: str

class DataResponse(BaseModel):
    symbol: str
    data: dict
    timestamp: datetime
    source: str

# Standard endpoints all services MUST have
@app.get("/health", response_model=HealthResponse)
async def health_check():
    return HealthResponse(
        status="healthy",
        timestamp=datetime.utcnow(),
        service="your-service",
        version="1.0.0"
    )

@app.get("/metrics")
async def metrics():
    return {
        "service": "your-service",
        "status": "operational",
        "uptime_seconds": get_uptime(),
        "requests_total": get_request_count(),
        "errors_total": get_error_count()
    }

# Data endpoints follow RESTful patterns
@app.get("/api/v1/data/{symbol}", response_model=DataResponse)
async def get_data(symbol: str):
    # Implementation
    pass

@app.get("/api/v1/data", response_model=List[DataResponse])
async def get_all_data():
    # Implementation
    pass
```

### **External API Integration Pattern**
Use this pattern for external API calls:

```python
import aiohttp
import asyncio
from typing import Dict, List
import logging

class ExternalAPIClient:
    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers={'Authorization': f'Bearer {self.api_key}'},
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_data(self, endpoint: str, params: Dict = None) -> Dict:
        url = f"{self.base_url}/{endpoint}"
        try:
            async with self.session.get(url, params=params) as response:
                response.raise_for_status()
                return await response.json()
        except aiohttp.ClientError as e:
            logging.error(f"API request failed: {e}")
            raise HTTPException(status_code=503, detail="External API unavailable")
        except asyncio.TimeoutError:
            logging.error(f"API request timeout: {url}")
            raise HTTPException(status_code=504, detail="External API timeout")

# Usage pattern
async def collect_crypto_prices(symbols: List[str]) -> List[Dict]:
    async with ExternalAPIClient(api_key, "https://api.coingecko.com/api/v3") as client:
        tasks = []
        for symbol in symbols:
            task = client.fetch_data(f"simple/price", {"ids": symbol, "vs_currencies": "usd"})
            tasks.append(task)
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return [r for r in results if not isinstance(r, Exception)]
```

## ðŸ—„ï¸ **Database Schema & Query Patterns**

### **Standard Table Schemas**
Follow these patterns for database tables:

```sql
-- Price data table pattern
CREATE TABLE IF NOT EXISTS price_data (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    symbol VARCHAR(20) NOT NULL,
    price DECIMAL(20,8) NOT NULL,
    market_cap BIGINT,
    volume_24h BIGINT,
    price_change_24h DECIMAL(10,4),
    timestamp DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    created_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    INDEX idx_symbol_timestamp (symbol, timestamp),
    INDEX idx_timestamp (timestamp),
    UNIQUE KEY unique_symbol_timestamp (symbol, timestamp)
);

-- News/sentiment data pattern
CREATE TABLE IF NOT EXISTS news_data (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    title TEXT NOT NULL,
    content LONGTEXT,
    url VARCHAR(1000),
    source VARCHAR(100) NOT NULL,
    symbols JSON,  -- Array of related cryptocurrency symbols
    sentiment_score DECIMAL(5,4),  -- -1.0 to 1.0
    published_at DATETIME(6),
    collected_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    INDEX idx_source_published (source, published_at),
    INDEX idx_symbols_published (symbols, published_at),
    INDEX idx_sentiment (sentiment_score),
    FULLTEXT idx_content (title, content)
);

-- ML features pattern
CREATE TABLE IF NOT EXISTS ml_features (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    symbol VARCHAR(20) NOT NULL,
    timestamp DATETIME(6) NOT NULL,
    features JSON NOT NULL,  -- All ML features as JSON object
    feature_version VARCHAR(10) NOT NULL DEFAULT '1.0',
    created_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    INDEX idx_symbol_timestamp (symbol, timestamp),
    INDEX idx_feature_version (feature_version),
    UNIQUE KEY unique_symbol_timestamp_version (symbol, timestamp, feature_version)
);
```

### **Standard Query Patterns**
Use these query patterns for consistency:

```python
# Get latest data for symbol
def get_latest_price(symbol: str) -> Dict:
    query = """
    SELECT * FROM price_data 
    WHERE symbol = %s 
    ORDER BY timestamp DESC 
    LIMIT 1
    """
    
# Get time series data
def get_historical_prices(symbol: str, hours: int = 24) -> List[Dict]:
    query = """
    SELECT * FROM price_data 
    WHERE symbol = %s 
    AND timestamp >= DATE_SUB(NOW(), INTERVAL %s HOUR)
    ORDER BY timestamp ASC
    """
    
# Bulk insert pattern
def bulk_insert_prices(price_data: List[Dict]):
    query = """
    INSERT INTO price_data (symbol, price, market_cap, volume_24h, timestamp)
    VALUES (%s, %s, %s, %s, %s)
    ON DUPLICATE KEY UPDATE
    price = VALUES(price),
    market_cap = VALUES(market_cap),
    volume_24h = VALUES(volume_24h)
    """
    values = [(d['symbol'], d['price'], d['market_cap'], d['volume_24h'], d['timestamp']) 
              for d in price_data]
    
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        cursor.executemany(query, values)
        conn.commit()
    finally:
        cursor.close()
        conn.close()
```

## ðŸ”„ **Service Communication Patterns**

### **Internal Service Communication**
Use Kubernetes service discovery for internal communication:

```python
import aiohttp

# Service-to-service communication pattern
class ServiceClient:
    def __init__(self, service_name: str, namespace: str = "crypto-collectors"):
        self.base_url = f"http://{service_name}.{namespace}.svc.cluster.local:8080"
    
    async def get_health(self) -> Dict:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.base_url}/health") as response:
                return await response.json()
    
    async def get_data(self, endpoint: str) -> Dict:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.base_url}/api/v1/{endpoint}") as response:
                response.raise_for_status()
                return await response.json()

# Usage
prices_client = ServiceClient("crypto-prices-collector")
news_client = ServiceClient("crypto-news-collector")

# Cross-service data fetching
prices = await prices_client.get_data("prices/bitcoin")
news = await news_client.get_data("news/crypto/latest")
```

### **WebSocket Streaming Pattern**
Implement real-time data streaming:

```python
from fastapi import WebSocket, WebSocketDisconnect
import json
import asyncio

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    
    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_text(json.dumps(message))
            except:
                # Connection closed, remove it
                self.active_connections.remove(connection)

manager = ConnectionManager()

@app.websocket("/ws/prices")
async def websocket_prices(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            # Send periodic updates
            latest_prices = await get_latest_prices()
            await websocket.send_text(json.dumps(latest_prices))
            await asyncio.sleep(5)
    except WebSocketDisconnect:
        manager.disconnect(websocket)

# Background task to push updates
async def price_update_task():
    while True:
        latest_prices = await fetch_latest_prices()
        await manager.broadcast({
            "type": "price_update",
            "data": latest_prices,
            "timestamp": datetime.utcnow().isoformat()
        })
        await asyncio.sleep(5)
```

## ðŸ§ª **Testing Patterns**

### **Unit Testing Pattern**
```python
import pytest
import asyncio
from unittest.mock import Mock, patch
from your_service import DataCollector

@pytest.fixture
def collector():
    return DataCollector()

@pytest.mark.asyncio
async def test_data_collection(collector):
    # Mock external API
    with patch('aiohttp.ClientSession.get') as mock_get:
        mock_response = Mock()
        mock_response.json.return_value = {"bitcoin": {"usd": 50000}}
        mock_get.return_value.__aenter__.return_value = mock_response
        
        result = await collector.collect_data(["bitcoin"])
        assert result["bitcoin"]["usd"] == 50000

def test_database_connection():
    # Test database connectivity
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT 1")
    result = cursor.fetchone()
    assert result[0] == 1
    cursor.close()
    conn.close()

@pytest.mark.integration
async def test_api_endpoints(collector):
    # Test FastAPI endpoints
    from fastapi.testclient import TestClient
    client = TestClient(collector.app)
    
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"
```

### **Integration Testing Pattern**
```python
import pytest
import subprocess
import time

@pytest.fixture(scope="session")
def kubernetes_deployment():
    # Deploy test environment
    subprocess.run(["kubectl", "apply", "-f", "k8s/test/"], check=True)
    time.sleep(30)  # Wait for deployment
    
    yield
    
    # Cleanup
    subprocess.run(["kubectl", "delete", "-f", "k8s/test/"], check=True)

@pytest.mark.integration
def test_service_health(kubernetes_deployment):
    # Test service health in Kubernetes
    result = subprocess.run([
        "kubectl", "exec", "deployment/crypto-prices-collector", 
        "--", "curl", "http://localhost:8080/health"
    ], capture_output=True, text=True)
    
    assert result.returncode == 0
    health_data = json.loads(result.stdout)
    assert health_data["status"] == "healthy"
```

## ðŸ“Š **Monitoring & Observability Patterns**

### **Metrics Collection Pattern**
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time

# Define metrics
REQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')
ACTIVE_CONNECTIONS = Gauge('active_connections', 'Active database connections')
COLLECTION_ERRORS = Counter('collection_errors_total', 'Collection errors', ['source', 'error_type'])

# Middleware for automatic metrics
@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    REQUEST_DURATION.observe(duration)
    return response

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")

# Usage in collection functions
async def collect_data_with_metrics():
    try:
        data = await external_api.fetch()
        REQUEST_COUNT.labels(source='external_api', status='success').inc()
        return data
    except Exception as e:
        COLLECTION_ERRORS.labels(source='external_api', error_type=type(e).__name__).inc()
        raise
```

### **Structured Logging Pattern**
```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.logger = logging.getLogger(service_name)
        
        # Configure structured logging
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def log(self, level: str, message: str, **kwargs):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "service": self.service_name,
            "level": level,
            "message": message,
            **kwargs
        }
        self.logger.info(json.dumps(log_entry))
    
    def info(self, message: str, **kwargs):
        self.log("INFO", message, **kwargs)
    
    def error(self, message: str, **kwargs):
        self.log("ERROR", message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        self.log("WARNING", message, **kwargs)

# Usage
logger = StructuredLogger("crypto-prices-collector")

async def collect_prices():
    logger.info("Starting price collection", symbols=["BTC", "ETH"])
    try:
        prices = await fetch_prices()
        logger.info("Price collection successful", 
                   count=len(prices), 
                   duration_ms=duration)
    except Exception as e:
        logger.error("Price collection failed", 
                    error=str(e), 
                    error_type=type(e).__name__)
```

## ðŸ”’ **Security & Configuration Patterns**

### **Environment Configuration Pattern**
```python
import os
from typing import Optional
from pydantic import BaseSettings

class Settings(BaseSettings):
    # Database settings
    mysql_host: str = "host.docker.internal"
    mysql_port: int = 3306
    mysql_user: str = "news_collector"
    mysql_password: str = "99Rules!"
    mysql_database: str = "crypto_prices"
    
    # API keys
    coingecko_api_key: Optional[str] = None
    fred_api_key: Optional[str] = None
    guardian_api_key: Optional[str] = None
    openai_api_key: Optional[str] = None
    
    # Service settings
    log_level: str = "INFO"
    collection_interval: int = 300  # 5 minutes
    max_retries: int = 3
    request_timeout: int = 30
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()

# Validation
if not settings.coingecko_api_key:
    raise ValueError("COINGECKO_API_KEY environment variable is required")
```

### **Rate Limiting Pattern**
```python
import asyncio
from collections import defaultdict
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self):
        self.requests = defaultdict(list)
    
    async def wait_if_needed(self, key: str, max_requests: int, time_window: int):
        now = datetime.utcnow()
        window_start = now - timedelta(seconds=time_window)
        
        # Clean old requests
        self.requests[key] = [
            req_time for req_time in self.requests[key] 
            if req_time > window_start
        ]
        
        # Check if we need to wait
        if len(self.requests[key]) >= max_requests:
            oldest_request = min(self.requests[key])
            wait_time = (oldest_request + timedelta(seconds=time_window) - now).total_seconds()
            if wait_time > 0:
                await asyncio.sleep(wait_time)
        
        # Record this request
        self.requests[key].append(now)

rate_limiter = RateLimiter()

async def api_call_with_rate_limit(api_name: str):
    await rate_limiter.wait_if_needed(api_name, max_requests=50, time_window=60)
    # Make API call
    return await external_api.fetch()
```

## ðŸŽ¯ **Critical Guidelines**

### **NEVER Do These Things**
1. **Never run services directly with Python** - Always use Kubernetes deployments
2. **Never use localhost or 127.0.0.1** for database connections - Use `host.docker.internal`
3. **Never hardcode API keys** - Always use Kubernetes secrets
4. **Never include trading logic** - This is a data collection system only
5. **Never deploy without health checks** - All services must have `/health` endpoints
6. **Never ignore resource limits** - Always set CPU/memory limits in Kubernetes

### **ALWAYS Do These Things**
1. **Always use structured logging** with JSON format
2. **Always implement proper error handling** with retries and exponential backoff
3. **Always validate external API responses** before storing in database
4. **Always use connection pooling** for database connections
5. **Always include metrics endpoints** for monitoring
6. **Always follow the service naming convention**: `{data-type}-{service-type}` (e.g., `crypto-prices-collector`)

### **Database Schema Guidelines**
1. **Always use DECIMAL for prices** - Never use FLOAT for financial data
2. **Always include timestamps** with microsecond precision (DATETIME(6))
3. **Always create appropriate indexes** for query performance
4. **Always use JSON columns** for flexible schema fields
5. **Always implement proper constraints** to prevent duplicate data

### **API Design Guidelines**
1. **Always version APIs** - Use `/api/v1/` prefix
2. **Always return consistent response formats** with proper HTTP status codes
3. **Always implement pagination** for list endpoints
4. **Always include rate limiting** for external-facing APIs
5. **Always provide OpenAPI documentation** via FastAPI automatic docs

These instructions should help you understand and work effectively with the crypto data collection system architecture and conventions.