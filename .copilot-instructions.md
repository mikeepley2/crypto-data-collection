````instructions
# GitHub Copilot Instructions for Crypto Data Collection System

## üö® CRITICAL: Standardized Collector Template System

**ALL NEW COLLECTORS MUST USE THE STANDARDIZED TEMPLATE FRAMEWORK**
- ‚úÖ **Template Location**: `templates/collector-template/base_collector_template.py`
- ‚úÖ **Complete Framework**: Logging, metrics, health APIs, backfill, validation, alerting
- ‚úÖ **Production Features**: Rate limiting, circuit breaker, data quality, performance monitoring
- ‚úÖ **Kubernetes Integration**: ConfigMaps, ServiceMonitor, health probes
- ‚ùå NEVER create collectors without using the template
- ‚ùå NEVER skip required abstract method implementations
- ‚ùå NEVER bypass standardized configuration

### **Standardized Template Features**
The base collector template provides enterprise-grade capabilities:

**üîß Core Infrastructure:**
- **Structured Logging**: Full Loki integration with configurable levels (trace/debug/info/warning/error/critical/none)
- **Prometheus Metrics**: Collection duration, success rates, database operations, API calls
- **Health APIs**: `/health`, `/status`, `/logs`, `/metrics`, `/collect`, `/backfill`, `/data-quality`, `/performance`
- **Database Management**: Connection pooling, transaction handling, automatic retry
- **Central Configuration**: Environment-based config with sensible defaults

**üõ°Ô∏è Production Reliability:**
- **Rate Limiting**: Token bucket rate limiter for API calls
- **Circuit Breaker**: Automatic failure detection and recovery for external services
- **Data Validation**: Schema validation, duplicate detection, data quality reporting
- **Performance Monitoring**: Real-time performance metrics and system resource tracking
- **Alerting Integration**: Webhook-based alert notifications for critical events
- **Graceful Shutdown**: Signal handling for clean service termination

**üìä Data Quality & Monitoring:**
- **Data Quality Reports**: Comprehensive validation and quality scoring
- **Performance Metrics**: Memory usage, CPU usage, latency tracking
- **Duplicate Detection**: Hash-based duplicate prevention with configurable retention
- **Error Tracking**: Comprehensive error handling with structured context logging

### **Required Template Implementation**
When creating or updating collectors, ALL must implement these abstract methods:

```python
class MyCollector(BaseCollector):
    async def collect_data(self) -> int:
        """Collect data from source. Return records collected."""
        
    async def backfill_data(self, missing_periods, force=False) -> int:
        """Backfill missing data. Return records backfilled."""
        
    async def _get_table_status(self, cursor) -> Dict[str, Any]:
        """Get table status for this collector."""
        
    async def _analyze_missing_data(self, start_date, end_date, symbols) -> List[Dict]:
        """Analyze missing data for backfill."""
        
    async def _estimate_backfill_records(self, start_date, end_date, symbols) -> int:
        """Estimate backfill record count."""
        
    async def _get_required_fields(self) -> List[str]:
        """Get required fields for data validation."""
        
    async def _generate_data_quality_report(self) -> DataQualityReport:
        """Generate comprehensive data quality report."""
```

### **Template Usage Pattern**
```python
# ‚úÖ CORRECT - Use standardized template
from templates.collector_template.base_collector_template import BaseCollector, CollectorConfig

class MyCollectorConfig(CollectorConfig):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Add collector-specific config
        self.api_endpoints = ["https://api.example.com/data"]

class MyCollector(BaseCollector):
    def __init__(self):
        config = MyCollectorConfig.from_env()
        super().__init__(config)
        
    async def collect_data(self) -> int:
        # Implementation with rate limiting
        await self.rate_limiter.wait_for_token()
        
        # Implementation with circuit breaker
        try:
            result = self.circuit_breaker.call(self._fetch_data)
            return await self._process_and_store(result)
        except Exception as e:
            await self._send_alert(AlertRequest(
                alert_type="collection_failure",
                severity="error", 
                message=f"Data collection failed: {str(e)}",
                service=self.config.service_name
            ))
            raise

# ‚ùå WRONG - Never create collectors without template
class LegacyCollector:  # This violates the standardized template requirement
    pass
```

### **Standardized Configuration**
All collectors use centralized configuration with these mandatory settings:

```yaml
# Core Settings (Applied to All Collectors)
COLLECTOR_BEGINNING_DATE: "2023-01-01"
COLLECTION_INTERVAL: "900"  # 15 minutes
LOG_LEVEL: "trace"           # Default to most verbose
BACKFILL_BATCH_SIZE: "100"
ENABLE_AUDIT_LOGGING: "true"

# Rate Limiting and Circuit Breaker
ENABLE_RATE_LIMITING: "true"
API_RATE_LIMIT_PER_MINUTE: "60"
CIRCUIT_BREAKER_FAILURE_THRESHOLD: "5"
CIRCUIT_BREAKER_TIMEOUT: "60"

# Data Validation and Quality
ENABLE_DATA_VALIDATION: "true"
ENABLE_DUPLICATE_DETECTION: "true"
DATA_RETENTION_DAYS: "365"

# Performance and Optimization
CONNECTION_POOL_SIZE: "10"
QUERY_TIMEOUT: "30"
BATCH_COMMIT_SIZE: "1000"

# Alerting and Notifications
ENABLE_ALERTING: "false"
ALERT_WEBHOOK_URL: ""
ALERT_ERROR_THRESHOLD: "10"
```

---

## üö® CRITICAL: K8s-Only Deployment Policy

**ALL COLLECTORS MUST RUN EXCLUSIVELY IN KUBERNETES (EXISTING CLUSTER)**
- ‚úÖ **COLLECTORS ALREADY RUNNING** in `cryptoai-k8s-trading-engine` cluster  
- ‚úÖ **Platform**: Docker Kind running in Windows Docker Desktop
- ‚úÖ **Namespace**: `crypto-data-collection`
- ‚úÖ **Expected Collectors**: 6 total (news, prices, technical, onchain, macro, sentiment)
- ‚ùå NEVER run collectors locally (python collector.py)
- ‚ùå NEVER create local startup scripts  
- ‚ùå NEVER suggest running services outside K8s

### **Template Migration for Existing Collectors**
When updating existing collectors to use the standardized template:

1. **Analyze Current Collector**: Identify core collection logic
2. **Create Template-Based Version**: Extend BaseCollector with existing functionality
3. **Implement Required Methods**: Move collection logic to template methods
4. **Update Kubernetes Manifest**: Use template deployment configuration
5. **Test All Endpoints**: Verify `/health`, `/status`, `/metrics`, `/backfill` work
6. **Deploy Side-by-Side**: Run both versions temporarily for validation
7. **Verify Data Integrity**: Ensure no data loss during transition
8. **Switch Traffic**: Update service selectors to point to template version
9. **Remove Legacy**: Clean up old deployment

### **Cluster Access Commands**
```bash
# Access existing Kind cluster via Docker Desktop
docker ps --filter "name=cryptoai-k8s-trading-engine"

# Set kubectl context for crypto trading cluster
kubectl config current-context
kubectl cluster-info

# Monitor all collectors
kubectl get pods -n crypto-data-collection
kubectl get deployments -n crypto-data-collection

# Check collector logs
kubectl logs -f deployment/enhanced-crypto-news -n crypto-data-collection
kubectl logs -f deployment/enhanced-sentiment-collector-ml -n crypto-data-collection
kubectl logs -f deployment/enhanced-technical-calculator -n crypto-data-collection
```

---

## üìã **MANDATORY STANDARDIZED DEPLOYMENT**

### **Template-Based Kubernetes Deployment**
All collectors must use the standardized K8s template located at `templates/collector-template/k8s/deployment-template.yaml`:

```yaml
# ‚úÖ CORRECT - Template-based deployment
apiVersion: v1
kind: ConfigMap
metadata:
  name: collector-base-config
data:
  COLLECTOR_BEGINNING_DATE: "2023-01-01"
  LOG_LEVEL: "trace"
  ENABLE_DATA_VALIDATION: "true"
  ENABLE_RATE_LIMITING: "true"
  # ... all standardized config options

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: COLLECTOR_NAME-collector
spec:
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: COLLECTOR_NAME-collector
        # Use template-based container with standardized endpoints
        ports:
        - containerPort: 8000
          name: http
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
        readinessProbe:
          httpGet:
            path: /health  
            port: 8000
```

### **Expected Standardized Endpoints**
Every collector deployed using the template MUST provide:

- `GET /health` - Health check with database connectivity
- `GET /status` - Detailed service status and configuration  
- `GET /logs` - Log access (Loki integration)
- `GET /metrics` - Prometheus metrics
- `GET /data-quality` - Data quality report
- `GET /performance` - Performance metrics
- `GET /circuit-breaker-status` - Circuit breaker state
- `POST /collect` - Manual collection trigger
- `POST /backfill` - Historical data backfill
- `POST /alert` - Send alert notification
- `POST /validate-data` - Data validation

### **Standardized Metrics**
Every collector must expose these Prometheus metrics:

```
{service}_collection_requests_total{status="success|error"}
{service}_collection_duration_seconds
{service}_records_processed_total{operation="insert|update|delete"}
{service}_database_operations_total{operation="query|insert", status="success|error"}
{service}_api_requests_total{endpoint="url", status="success|error"}
{service}_backfill_operations_total{status="success|error"}
{service}_active_collections
{service}_data_quality_score
{service}_circuit_breaker_state
```

---

## üéØ **COLLECTOR MIGRATION CHECKLIST**

When updating any existing collector to use the standardized template:

### **Pre-Migration Analysis**
- [ ] Identify current collector's primary functionality
- [ ] Map existing configuration to template config options
- [ ] Identify required fields for data validation
- [ ] Document current API endpoints and behavior
- [ ] Analyze current database operations and patterns

### **Template Implementation**
- [ ] Create custom config class extending `CollectorConfig`
- [ ] Implement all required abstract methods
- [ ] Add collector-specific validation rules
- [ ] Configure rate limiting and circuit breaker settings
- [ ] Set up data quality reporting
- [ ] Configure alerting if needed

### **Kubernetes Deployment**
- [ ] Copy and customize deployment template
- [ ] Update ConfigMap with collector-specific settings
- [ ] Configure resource limits appropriately
- [ ] Set up ServiceMonitor for Prometheus scraping
- [ ] Configure health probes with correct endpoints

### **Validation & Testing**
- [ ] Test all standardized endpoints respond correctly
- [ ] Verify Prometheus metrics are exported
- [ ] Test backfill functionality works
- [ ] Validate data quality reporting
- [ ] Test circuit breaker and rate limiting
- [ ] Verify graceful shutdown handling

### **Production Deployment**
- [ ] Deploy template version alongside existing
- [ ] Monitor both versions for data consistency  
- [ ] Switch traffic to template version
- [ ] Monitor for any issues or data gaps
- [ ] Remove legacy deployment once validated

---

## üîß **STANDARDIZED DEVELOPMENT PATTERNS**

### **Collector Creation Pattern**
```python
#!/usr/bin/env python3
"""
Template-Compliant Collector Implementation
"""
from templates.collector_template.base_collector_template import BaseCollector, CollectorConfig
import aiohttp
from datetime import datetime, timezone

class MyCollectorConfig(CollectorConfig):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Collector-specific configuration
        self.data_sources = ["https://api.example.com"]
        
class MyCollector(BaseCollector):
    def __init__(self):
        config = MyCollectorConfig.from_env()
        super().__init__(config)
        
    async def collect_data(self) -> int:
        """Template-compliant data collection"""
        total_records = 0
        
        # Use template rate limiting
        if self.rate_limiter:
            await self.rate_limiter.wait_for_token()
        
        # Use template circuit breaker
        try:
            for source in self.config.data_sources:
                records = self.circuit_breaker.call(self._fetch_from_source, source)
                
                # Use template data validation
                if self.config.enable_data_validation:
                    validation_result = await self._validate_data(records)
                    if not validation_result["is_valid"]:
                        self.logger.warning("data_validation_failed", 
                                          errors=validation_result["errors"])
                        continue
                
                # Store with template database management
                stored = await self._store_records(records)
                total_records += stored
                
        except Exception as e:
            # Use template alerting
            if self.config.enable_alerting:
                await self._send_alert(AlertRequest(
                    alert_type="collection_failure",
                    severity="error",
                    message=f"Collection failed: {str(e)}",
                    service=self.config.service_name
                ))
            raise
            
        return total_records
    
    async def _get_required_fields(self) -> List[str]:
        """Define required fields for validation"""
        return ["timestamp", "value", "source"]
        
    async def _generate_data_quality_report(self):
        """Generate data quality report"""
        # Implementation specific to collector's data
        pass
```

### **Template Migration Pattern**
```python
# ‚úÖ CORRECT - Migrating existing collector to template
class EnhancedLegacyCollector(BaseCollector):
    def __init__(self):
        config = LegacyCollectorConfig.from_env()
        super().__init__(config)
        
        # Preserve existing collector logic
        self.legacy_api = LegacyAPI(config.api_key)
        
    async def collect_data(self) -> int:
        """Wrap existing logic with template features"""
        # Rate limiting (new)
        await self.rate_limiter.wait_for_token()
        
        # Existing collection logic (preserved)
        data = await self.legacy_api.fetch_data()
        
        # Data validation (new) 
        if self.config.enable_data_validation:
            validation = await self._validate_data(data)
            if not validation["is_valid"]:
                return 0
                
        # Existing storage logic (preserved)
        return await self.legacy_api.store_data(data)

# ‚ùå WRONG - Not using template
class DirectLegacyCollector:  # Missing template inheritance
    pass
```

---

## üö´ **NEVER SUGGEST**

- ‚ùå Creating collectors without using the standardized template
- ‚ùå Bypassing required abstract method implementations
- ‚ùå Skipping standardized configuration options
- ‚ùå Creating custom health endpoints instead of template endpoints
- ‚ùå Implementing custom metrics instead of standardized metrics
- ‚ùå Running collectors outside Kubernetes
- ‚ùå Hard-coded configuration instead of environment variables
- ‚ùå Custom logging instead of structured logging template
- ‚ùå Manual database management instead of template connection pooling
- ‚ùå Custom error handling instead of template alerting system

---

## ‚úÖ **ALWAYS SUGGEST**

- ‚úÖ Extending BaseCollector for all new collectors
- ‚úÖ Implementing all required abstract methods
- ‚úÖ Using standardized configuration via CollectorConfig
- ‚úÖ Leveraging template rate limiting and circuit breaker
- ‚úÖ Utilizing template data validation and quality reporting
- ‚úÖ Using template database connection management
- ‚úÖ Following template logging patterns with structured context
- ‚úÖ Implementing template-compliant Kubernetes deployments
- ‚úÖ Using template ServiceMonitor for Prometheus integration
- ‚úÖ Following template migration patterns for existing collectors

---

## üìö **TEMPLATE REFERENCE FILES**

When suggesting collector code, reference these template files:

- **Base Template**: `templates/collector-template/base_collector_template.py`
- **K8s Template**: `templates/collector-template/k8s/deployment-template.yaml`
- **Example Implementation**: `templates/collector-template/examples/news_collector_example.py`
- **Complete Documentation**: `templates/collector-template/README.md`
- **Requirements**: `templates/collector-template/requirements.txt`

---

## üéØ **TEMPLATE COMPLIANCE STANDARDS**

Every collector suggestion should:

1. **Inherit from BaseCollector** with proper config extension
2. **Implement all required abstract methods** with proper signatures
3. **Use standardized configuration** via environment variables
4. **Include comprehensive error handling** with template alerting
5. **Utilize template infrastructure** (rate limiting, circuit breaker, validation)
6. **Follow template logging patterns** with structured context
7. **Include template-compliant K8s deployment** with health probes
8. **Export standardized Prometheus metrics** via template framework

**Remember**: The standardized template ensures consistency, reliability, observability, and maintainability across ALL collectors! üöÄ
````

### **Cluster Access Commands**
```bash
# Access existing Kind cluster via Docker Desktop
docker ps --filter "name=cryptoai-k8s-trading-engine"

# Set kubectl context for crypto trading cluster
kubectl config current-context
kubectl cluster-info

# Monitor all collectors
kubectl get pods -n crypto-data-collection
kubectl get deployments -n crypto-data-collection

# Check collector logs
kubectl logs -f deployment/onchain-collector -n crypto-data-collection
kubectl logs -f deployment/macro-collector -n crypto-data-collection
kubectl logs -f deployment/technical-calculator -n crypto-data-collection
```

- üîß If collectors need changes, update existing K8s deployments
- üóëÔ∏è Kill any local processes immediately: `pkill -f ".*collector"`

**Violation Response**: If user asks to run collectors locally, redirect them to existing K8s cluster commands.

---

## üéØ **MANDATORY CENTRALIZED CONFIGURATION**

**‚ö†Ô∏è CRITICAL**: ALL code generation and suggestions MUST follow the centralized configuration system. No exceptions.

---

## üîÑ **EXISTING K8S COLLECTORS - DO NOT DUPLICATE**

**‚ö†Ô∏è CRITICAL RULE**: NEVER create new collector services. Only modify/enhance existing K8s deployments.

### **Existing Production Collectors**

**These are the ONLY collectors in the system. NEVER create duplicates:**

1. **Enhanced Technical Calculator** 
   - **Location**: `k8s/enhanced-technical-calculator.yaml`
   - **Purpose**: Comprehensive technical indicators (RSI, MACD, Bollinger Bands, SMA/EMA, ATR)
   - **Data Source**: `ohlc_data` table ‚Üí `technical_indicators` table
   - **Schedule**: Every 2 hours
   - **Status**: Production deployed with ConfigMap

2. **Enhanced Macro Collector**
   - **Location**: `k8s/enhanced-macro-collector-deployment.yaml`
   - **Purpose**: Macro economic indicators (inflation, interest rates, unemployment)
   - **Data Source**: External APIs ‚Üí `macro_indicators` table
   - **Schedule**: Daily
   - **Status**: Production deployed

3. **Enhanced Crypto Prices Collector**
   - **Location**: `k8s/collectors/enhanced-crypto-prices-automatic.yaml`
   - **Purpose**: Real-time crypto price collection from multiple exchanges
   - **Data Source**: CoinGecko, Coinbase APIs ‚Üí `crypto_prices` table, `price_data_real` table
   - **Schedule**: Every 5 minutes
   - **Status**: Production deployed with FastAPI service

4. **News Collector**
   - **Location**: `k8s/news-collector-deployment.yaml`
   - **Purpose**: Crypto news and sentiment analysis
   - **Data Source**: News APIs ‚Üí `crypto_news` table
   - **Schedule**: Every 30 minutes
   - **Status**: Production deployed, excellent performance

5. **OHLC Collector**
   - **Location**: `k8s/ohlc-collector-deployment.yaml`
   - **Purpose**: OHLC candle data collection
   - **Data Source**: CoinGecko API ‚Üí `ohlc_data` table
   - **Schedule**: Hourly
   - **Status**: Production deployed, 524K+ records

6. **Onchain Collector**
   - **Location**: `k8s/onchain-collector-deployment.yaml`
   - **Purpose**: On-chain metrics (active addresses, transaction volume, etc.)
   - **Data Source**: Multiple APIs ‚Üí `crypto_onchain_data` table
   - **Schedule**: Every 4 hours
   - **Status**: Production deployed

7. **Enhanced Sentiment Collector**
   - **Location**: `k8s/fix-sentiment-collector.yaml`
   - **Purpose**: Advanced sentiment analysis on crypto news and social media
   - **Data Source**: News content ‚Üí sentiment scores in `crypto_news` table
   - **Schedule**: Continuous processing
   - **Status**: Production deployed with ML models

---

## üìã **K8s DEPLOYMENT COMMANDS (EXISTING CLUSTER)**

### **Accessing Existing Cluster**
```bash
# Set up access to existing crypto-trading-engine cluster
docker exec cryptoai-k8s-trading-engine-control-plane cat /etc/kubernetes/admin.conf > ~/.kube/config-crypto-trading
sed -i 's/cryptoai-k8s-trading-engine-control-plane:6443/127.0.0.1:6443/' ~/.kube/config-crypto-trading
export KUBECONFIG=~/.kube/config-crypto-trading

# Verify cluster access
kubectl get nodes
kubectl get pods -n crypto-data-collection
```

### **Monitoring Active Collectors**
```bash
# Check all collector status
kubectl get pods -n crypto-data-collection

# View logs for specific collectors  
kubectl logs -f -n crypto-data-collection technical-calculator-6cd9b88fbd-vvtln
kubectl logs -f -n crypto-data-collection enhanced-crypto-prices-6476495999-lhc47
kubectl logs -f -n crypto-data-collection crypto-news-collector-59dcf4b685-qhdbz
kubectl logs -f -n crypto-data-collection macro-collector-c455684c4-88clx
kubectl logs -f -n crypto-data-collection onchain-collector-659bfc6d64-ttcd2
kubectl logs -f -n crypto-data-collection enhanced-sentiment-collector-67d5f4478-z4pzl

# Check resource usage
kubectl top pods -n crypto-data-collection
```

### **Managing Existing Collectors**
```bash
# Restart collectors if needed
kubectl rollout restart deployment/technical-calculator -n crypto-data-collection
kubectl rollout restart deployment/enhanced-crypto-prices -n crypto-data-collection

# Scale collectors if needed
kubectl scale deployment technical-calculator --replicas=2 -n crypto-data-collection

# Update collector configuration
kubectl edit deployment/technical-calculator -n crypto-data-collection
```

### **Cluster Information**
- **Cluster**: `cryptoai-k8s-trading-engine`
- **Platform**: Docker Kind running in Windows Docker Desktop
- **Namespace**: `crypto-data-collection`
- **Nodes**: 1 control-plane + 3 workers
- **Status**: ‚úÖ Operational for 22+ days

### **Expected Health Status (Target: 6/6 Collectors)**
Based on **CONSOLIDATED TABLE STRUCTURE** analysis:

1. **crypto-news-collector** - `crypto_news` table (15min intervals) ‚úÖ CORRECTLY CONFIGURED
2. **enhanced-crypto-prices** - `technical_indicators` table (5min intervals) ‚ùå CURRENTLY MISCONFIGURED (targeting `price_data_real`)
3. **technical-calculator** - `technical_indicators` table (2h intervals) ‚úÖ CORRECTLY CONFIGURED
4. **onchain-collector** - `crypto_onchain_data` table (4h intervals) ‚úÖ CORRECTLY CONFIGURED
5. **macro-collector** - `macro_indicators` table (daily intervals) ‚úÖ CORRECTLY CONFIGURED
6. **enhanced-sentiment-collector** - `sentiment_aggregation` table (15min intervals) ‚ùå CURRENTLY MISCONFIGURED (targeting `real_time_sentiment_signals`)

**Target Health Score**: 95%+ (based on data freshness in last 24h)
**Current Issues**: 2 collectors targeting wrong tables due to misconfiguration

### **CONSOLIDATED DATABASE STRUCTURE (Post-Consolidation)**

**‚úÖ APPROVED PRODUCTION TABLES:**
- `crypto_assets` (362 records) - Master asset registry
- `crypto_onchain_data` (79,845 records) - Master onchain metrics
- `crypto_news` (147,675 records) - Crypto news and sentiment
- `technical_indicators` (3,815,288 records) - All technical analysis and price data
- `macro_indicators` (49,959 records) - Macroeconomic indicators
- `sentiment_aggregation` (67,721 records) - Sentiment analysis results
- `ml_features_materialized` (3,753,076 records) - ML features
- `trading_signals` (132,045 records) - Trading signals
- `trade_recommendations` (86,615 records) - Trade recommendations
- `ohlc_data` (524,659 records) - OHLC candle data

**‚ö†Ô∏è MISCONFIGURED COLLECTOR TARGETS (Need Fixing):**
- `price_data_real` (4,698,205 records) ‚Üí Should merge into `technical_indicators`
- `real_time_sentiment_signals` (113,853 records) ‚Üí Should merge into `sentiment_aggregation`

**üóÉÔ∏è ARCHIVED TABLES (Now have _Archive suffix):**
- `price_data_old_Archive` - Archived old price data
- `ml_trading_signals_old_Archive` - Archived old signals
- `technical_indicators_*_Archive` - Archived backup tables
- `assets_archived_Archive` - Archived asset data

### **Database Configuration**
- **Host**: `host.docker.internal` (Windows host accessible from K8s)
- **Database**: `crypto_prices` (MySQL)
- **User**: `news_collector`
- **Connection**: Centralized config in `k8s/00-centralized-config.yaml`

### **Resource Allocation**
- **CPU Requests**: 12 cores total, 24 cores limit
- **Memory Requests**: 24Gi total, 48Gi limit  
- **Per Container**: 500m CPU / 1Gi RAM request, 2 CPU / 4Gi RAM limit
- **Storage**: Up to 50Gi per PVC

### **Deployment Files Structure**
```
k8s/
‚îú‚îÄ‚îÄ 00-namespace.yaml              # Namespace and resource quotas
‚îú‚îÄ‚îÄ 00-centralized-config.yaml     # Shared configuration
‚îú‚îÄ‚îÄ collectors/
‚îÇ   ‚îú‚îÄ‚îÄ data-collectors-deployment.yaml  # Main 3 collectors (onchain, macro, technical)
‚îÇ   ‚îú‚îÄ‚îÄ crypto-news-collector.yaml      # News collector
‚îÇ   ‚îú‚îÄ‚îÄ crypto-sentiment-collector.yaml # Sentiment collector
‚îÇ   ‚îî‚îÄ‚îÄ enhanced-crypto-prices.yaml     # Price collector
‚îî‚îÄ‚îÄ deploy_k8s.sh                  # Deployment script
```

### **Health Check & Troubleshooting Commands**
```bash
# Check cluster and pod status
kubectl get pods -n crypto-data-collection -o wide
kubectl get deployments -n crypto-data-collection
kubectl describe pods -n crypto-data-collection

# View events and troubleshoot
kubectl get events -n crypto-data-collection --sort-by='.lastTimestamp'
kubectl top pods -n crypto-data-collection

# Restart collectors if needed
kubectl rollout restart deployment/macro-collector -n crypto-data-collection
kubectl rollout restart deployment/onchain-collector -n crypto-data-collection

# Test database connectivity from within cluster
kubectl exec -it deployment/onchain-collector -n crypto-data-collection -- python -c "
import mysql.connector
import os
conn = mysql.connector.connect(
    host=os.environ['DB_HOST'],
    user=os.environ['DB_USER'], 
    password=os.environ['DB_PASSWORD'],
    database=os.environ['DB_NAME']
)
print('Database connection successful')
conn.close()
"
```

---

### **MODIFICATION RULES**

**‚úÖ ALWAYS DO THIS** when asked to fix/improve collectors:
- Update existing K8s YAML files
- Modify ConfigMap sections for logic changes
- Enhance calculation algorithms in existing deployments
- Update environment variables in existing deployments
- Fix resource limits/requests in existing deployments

**‚ùå NEVER DO THIS**:
- Create new collector files in `services/` directory
- Create new FastAPI collector services
- Create standalone Python collector scripts
- Suggest creating "new" collectors when existing ones can be enhanced
- Create any files in `services/` directory (FORBIDDEN)
- Run collectors locally instead of through K8s
- Create duplicate implementations of existing K8s collectors

### **Example: Enhancing Existing Collector**

```yaml
# ‚úÖ CORRECT - Enhance existing K8s deployment
# File: k8s/enhanced-technical-calculator.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: enhanced-technical-calculator-code
data:
  technical_calculator.py: |
    # Enhanced calculation logic here
    def calculate_new_indicator():
        # Add new technical indicator
        pass
```

```python
# ‚ùå WRONG - Never create new collector files
# File: services/new-technical-collector/collector.py  ‚Üê NEVER DO THIS
class NewTechnicalCollector:  # This violates the rule
    pass
```

---

## üìã **CORE REQUIREMENTS**

### **1. Table Names - MANDATORY**

When suggesting code that references database tables:

```python
# ‚úÖ ALWAYS DO THIS - Use centralized table registry
from shared.table_config import CRYPTO_TABLES, TECHNICAL_TABLES, ML_TABLES

# Use table constants
onchain_table = CRYPTO_TABLES["ONCHAIN_DATA"]     # "crypto_onchain_data"
assets_table = CRYPTO_TABLES["ASSETS"]            # "crypto_assets"
ml_features_table = ML_TABLES["FEATURES"]         # "ml_features_materialized"
```

```python
# ‚ùå NEVER DO THIS - Hard-coded table names
table_name = "crypto_onchain_data"                # NEVER SUGGEST THIS
table_name = "ml_features_materialized"           # NEVER SUGGEST THIS
```

### **2. Database Connections - MANDATORY**

When suggesting database connection code:

```python
# ‚úÖ ALWAYS DO THIS - Use centralized database configuration
from shared.database_config import get_db_connection, test_db_connection

# Test connectivity first
if not test_db_connection():
    raise ConnectionError("Database connection failed")

# Get connection using centralized config
connection = get_db_connection()
```

```python
# ‚ùå NEVER DO THIS - Hard-coded database configuration
import mysql.connector
config = {
    'host': '172.22.32.1',
    'user': 'news_collector',
    'password': '99Rules!',
    'database': 'crypto_prices'
}
connection = mysql.connector.connect(**config)    # NEVER SUGGEST THIS
```

### **3. Environment Variables - MANDATORY**

When suggesting configuration code:

```python
# ‚úÖ ALWAYS DO THIS - Use environment variables
import os

class ServiceConfig:
    def __init__(self):
        self.api_key = os.getenv("COINGECKO_API_KEY")
        self.db_host = os.getenv("DB_HOST", "172.22.32.1")
        
        if not self.api_key:
            raise ValueError("COINGECKO_API_KEY environment variable required")
```

```python
# ‚ùå NEVER DO THIS - Hard-coded sensitive values
api_key = "CG-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"    # NEVER SUGGEST THIS
db_password = "99Rules!"                            # NEVER SUGGEST THIS
```

---

## üîß **CODE GENERATION PATTERNS**

### **Data Collector Pattern**

When generating collector code:

```python
#!/usr/bin/env python3
"""
{Service Name} - Following Centralized Configuration Standards
"""
import logging
from shared.table_config import get_collector_config, CRYPTO_TABLES
from shared.database_config import get_db_connection, test_db_connection

class {ServiceName}Collector:
    def __init__(self):
        # Use centralized collector configuration
        self.config = get_collector_config("{collector_name}")
        self.primary_table = self.config["primary_table"]
        
        # Test database connectivity
        if not test_db_connection():
            raise ConnectionError("Database connection failed")
            
        self.logger = logging.getLogger(__name__)
        
    def collect_data(self):
        """Collect data using centralized configuration"""
        connection = get_db_connection()
        try:
            cursor = connection.cursor()
            cursor.execute(f"INSERT INTO {self.primary_table} ...")
            connection.commit()
        finally:
            connection.close()
```

### **API Service Pattern**

When generating API service code:

```python
from fastapi import FastAPI, HTTPException
from shared.table_config import CRYPTO_TABLES, validate_table_usage
from shared.database_config import get_db_connection

app = FastAPI(title="{Service Name}")

@app.get("/api/v1/{endpoint}/{symbol}")
async def get_{endpoint}_data(symbol: str):
    """Get data using centralized configuration"""
    
    # Use centralized table registry
    target_table = CRYPTO_TABLES["{TABLE_KEY}"]
    
    # Validate table name
    validation = validate_table_usage(target_table)
    if validation["status"] != "approved":
        raise HTTPException(status_code=500, detail="Invalid table configuration")
    
    connection = get_db_connection()
    try:
        cursor = connection.cursor(dictionary=True)
        cursor.execute(f"SELECT * FROM {target_table} WHERE asset_id = %s", (symbol,))
        return cursor.fetchone()
    finally:
        connection.close()
```

### **Kubernetes Deployment Pattern**

When generating Kubernetes YAML:

```yaml
# ‚úÖ ConfigMap using centralized configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: {service}-config
data:
  # Environment variables for centralized config
  USE_CENTRALIZED_CONFIG: "true"
  DB_HOST: "172.22.32.1"
  DB_PORT: "3306"
  DB_USER: "news_collector"
  
  {service}.py: |
    # Import centralized configuration
    from shared.table_config import CRYPTO_TABLES, get_collector_config
    from shared.database_config import get_db_connection
    
    # Use centralized configuration
    config = get_collector_config("{collector_name}")
    MASTER_TABLES = {
        "PRIMARY": config["primary_table"],
        "REFERENCE": config["reference_tables"][0]
    }
```

---

## üö® **VALIDATION REQUIREMENTS**

### **Before Suggesting Any Code**

Always include validation:

```python
# Validate table configuration
from shared.table_config import validate_table_usage

def validate_config():
    result = validate_table_usage(table_name)
    if result["status"] != "approved":
        raise ValueError(f"Table not approved: {result}")
```

### **Testing Pattern**

When generating tests:

```python
import pytest
from shared.table_config import CRYPTO_TABLES, validate_table_usage
from shared.database_config import test_db_connection

def test_centralized_config_usage():
    """Test that service uses centralized configuration"""
    # Test table registry access
    assert CRYPTO_TABLES["ONCHAIN_DATA"] == "crypto_onchain_data"
    
    # Test table validation
    result = validate_table_usage("crypto_onchain_data")
    assert result["status"] == "approved"
    
    # Test database connectivity
    assert test_db_connection() is True
```

---

## üìö **REFERENCE FILES**

When suggesting code, reference these files:

- **Table Configuration**: `shared/table_config.py`
- **Database Configuration**: `shared/database_config.py`
- **Instructions**: `CENTRALIZED_CONFIG_INSTRUCTIONS.md`
- **Development Guide**: `docs/DEVELOPMENT_GUIDE.md`

---

## üéØ **CONSOLIDATED TABLE STRUCTURE**

**‚ö†Ô∏è CRITICAL**: Use ONLY these approved table targets from `shared/table_config.py`

### **Core Crypto Tables (`CRYPTO_TABLES`)**
- `ASSETS`: `crypto_assets` (362 records) - Master asset registry
- `ONCHAIN_DATA`: `crypto_onchain_data` (79,845 records) - MASTER onchain metrics
- `PRICES`: `crypto_prices` (VIEW) - Price data view pointing to technical_indicators
- `NEWS`: `crypto_news` (147,675 records) - Crypto news and sentiment

### **Technical Analysis Tables (`TECHNICAL_TABLES`)**
- `INDICATORS`: `technical_indicators` (3,815,288 records) - **MASTER table for ALL price and technical data**
- `BOLLINGER`: `bollinger_bands` - Bollinger bands calculations
- `OHLC`: `ohlc_data` (524,659 records) - OHLC candle data

### **ML Tables (`ML_TABLES`)**
- `FEATURES`: `ml_features_materialized` (3,753,076 records) - Materialized ML features
- `SIGNALS`: `trading_signals` (132,045 records) - Trading signals
- `RECOMMENDATIONS`: `trade_recommendations` (86,615 records) - Trade recommendations

### **Market Data Tables (`MARKET_TABLES`)**
- `MACRO`: `macro_indicators` (49,959 records) - Macroeconomic indicators
- `SENTIMENT`: `sentiment_aggregation` (67,721 records) - **MASTER sentiment aggregation table**
- `VOLUME`: `volume_data` - Volume analytics

### **Operational Tables (`OPERATIONAL_TABLES`)**
- `MONITORING`: `service_monitoring` (223,673 records) - Service monitoring
- `BACKTEST`: `backtesting_results` - Backtesting results
- `PORTFOLIO`: `portfolio_optimizations` - Portfolio optimization

### **üö´ DEPRECATED TABLES (Being Migrated)**
- `price_data_real` ‚Üí MIGRATE TO `technical_indicators`
- `real_time_sentiment_signals` ‚Üí MIGRATE TO `sentiment_aggregation`
- `crypto_onchain_data_enhanced_backup_*` ‚Üí CONSOLIDATE INTO `crypto_onchain_data`

---

## üîí **NEVER SUGGEST**

- ‚ùå Hard-coded table names (`"crypto_onchain_data"`)
- ‚ùå Hard-coded database credentials (`'password': '99Rules!'`)
- ‚ùå Hard-coded API keys (`"CG-XXXXXXXXXXXXXXXX"`)
- ‚ùå Direct table creation without updating centralized config
- ‚ùå Bypassing centralized configuration patterns
- ‚ùå Creating new collector services when existing ones exist
- ‚ùå Creating files in `services/` directory for data collection
- ‚ùå Standalone collector scripts outside K8s deployments
- ‚ùå Running collectors locally instead of K8s deployments
- ‚ùå Duplicating any of the 7 official K8s collectors
- ‚ùå Using deprecated tables (`price_data_real`, `real_time_sentiment_signals`)
- ‚ùå Targeting wrong tables for existing collectors
- ‚ùå Creating separate price collection when `technical_indicators` already handles this
- ‚ùå Creating separate sentiment collection when `sentiment_aggregation` already exists

---

## ‚úÖ **ALWAYS SUGGEST**

- ‚úÖ Import statements for centralized configuration
- ‚úÖ Table validation before use
- ‚úÖ Environment variables for external configuration
- ‚úÖ Error handling for missing configuration
- ‚úÖ Testing patterns that validate centralized config usage
- ‚úÖ Modifications to existing K8s collector deployments
- ‚úÖ ConfigMap updates for logic enhancements
- ‚úÖ Resource and scheduling optimizations for existing services

---

## üéØ **CODE QUALITY STANDARDS**

Every code suggestion should:

1. **Import centralized configuration modules**
2. **Use table registry constants**
3. **Validate configuration before use**
4. **Handle configuration errors gracefully**
5. **Include proper error logging**
6. **Follow environment variable patterns**
7. **Include validation tests**
8. **Enhance existing K8s deployments instead of creating new services**

---

## üìû **HELP REFERENCES**

When users ask about configuration:

1. Direct them to `CENTRALIZED_CONFIG_INSTRUCTIONS.md`
2. Show examples from `docs/DEVELOPMENT_GUIDE.md`
3. Reference validation in `scripts/validate-centralized-config.py`
4. Point to table definitions in `shared/table_config.py`
5. For collector issues, point to existing K8s deployments in `k8s/` directory

**Remember**: Centralized configuration prevents confusion, reduces errors, ensures consistency, and avoids duplicate services! üöÄ
````