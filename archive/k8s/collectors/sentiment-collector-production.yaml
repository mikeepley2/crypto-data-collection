apiVersion: apps/v1
kind: Deployment
metadata:
  name: sentiment-collector
  namespace: crypto-data-collection
  labels:
    app: sentiment-collector
    component: data-collector
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/metrics"
    prometheus.io/port: "8000"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sentiment-collector
  template:
    metadata:
      labels:
        app: sentiment-collector
        component: data-collector
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      nodeSelector:
        node-type: data-collection # Target the data collection node
      tolerations:
        - key: "data-platform"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      containers:
        - name: sentiment-collector
          image: python:3.11-slim
          command: ["/bin/bash", "-c"]
          args:
            - |
              # Install dependencies
              pip install fastapi uvicorn textblob requests tenacity prometheus-client circuitbreaker redis mysql-connector-python

              # Create app directory
              mkdir -p /app

              # Create the production sentiment collector service
              cat > /app/sentiment_collector.py << 'EOF'
              import os
              import sys
              import json
              import time
              import logging
              from datetime import datetime, timedelta
              from typing import List, Dict, Optional, Tuple
              from dataclasses import dataclass

              import requests
              from requests.adapters import HTTPAdapter
              from urllib3.util.retry import Retry
              from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
              from circuitbreaker import CircuitBreaker
              from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
              import mysql.connector
              import redis
              from fastapi import FastAPI, BackgroundTasks, HTTPException
              from fastapi.responses import Response, JSONResponse
              import uvicorn

              # TextBlob for sentiment analysis
              try:
                  from textblob import TextBlob
                  TEXTBLOB_AVAILABLE = True
              except ImportError:
                  TEXTBLOB_AVAILABLE = False
                  print("Warning: TextBlob not available, using fallback sentiment analysis")

              # Set up structured logging
              logging.basicConfig(
                  level=logging.INFO,
                  format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
              )
              logger = logging.getLogger(__name__)

              # Prometheus metrics
              SENTIMENT_ANALYSIS_TOTAL = Counter(
                  'sentiment_analysis_total',
                  'Total sentiment analyses performed',
                  ['source', 'model']
              )

              SENTIMENT_ANALYSIS_DURATION = Histogram(
                  'sentiment_analysis_duration_seconds',
                  'Sentiment analysis duration',
                  ['model']
              )

              SENTIMENT_SCORE_DISTRIBUTION = Histogram(
                  'sentiment_score_distribution',
                  'Distribution of sentiment scores',
                  ['source', 'model'],
                  buckets=[-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
              )

              SENTIMENT_SOURCES_PROCESSED = Counter(
                  'sentiment_sources_processed_total',
                  'Total sentiment sources processed',
                  ['source']
              )

              SENTIMENT_ERRORS = Counter(
                  'sentiment_errors_total',
                  'Total sentiment analysis errors',
                  ['source', 'error_type']
              )

              SENTIMENT_CACHE_HITS = Counter(
                  'sentiment_cache_hits_total',
                  'Total sentiment cache hits',
                  ['source']
              )

              @dataclass
              class SentimentResult:
                  text: str
                  sentiment_score: float
                  confidence: float
                  model_used: str
                  source: str
                  processed_at: datetime

              class SentimentCollector:
                  def __init__(self):
                      self.app = FastAPI(
                          title="Sentiment Collector",
                          description="Production-grade cryptocurrency sentiment analysis service",
                          version="2.0.0"
                      )
                      self.setup_routes()
                      
                      # Sentiment analysis models
                      self.models = {
                          "textblob": self.analyze_with_textblob,
                          "vader": self.analyze_with_vader_fallback,
                          "rule_based": self.analyze_with_rules
                      }
                      
                      # Circuit breakers for external services
                      self.circuit_breakers = {
                          "database": CircuitBreaker(failure_threshold=5, recovery_timeout=60),
                          "redis": CircuitBreaker(failure_threshold=5, recovery_timeout=60)
                      }
                      
                      # HTTP session with retry strategy
                      self.session = requests.Session()
                      retry_strategy = Retry(
                          total=3,
                          backoff_factor=1,
                          status_forcelist=[429, 500, 502, 503, 504],
                      )
                      adapter = HTTPAdapter(max_retries=retry_strategy)
                      self.session.mount("http://", adapter)
                      self.session.mount("https://", adapter)
                      
                      # Redis connection for caching
                      try:
                          self.redis_client = redis.Redis(
                              host=os.getenv("REDIS_HOST", "redis-data-collection.crypto-data-collection.svc.cluster.local"),
                              port=int(os.getenv("REDIS_PORT", "6379")),
                              decode_responses=True
                          )
                          self.redis_client.ping()
                          logger.info("âœ… Connected to Redis for sentiment caching")
                      except Exception as e:
                          logger.warning(f"âš ï¸ Redis connection failed: {e}")
                          self.redis_client = None
                      
                      # Stats
                      self.stats = {
                          "total_analyzed": 0,
                          "total_updated": 0,
                          "last_analysis": None,
                          "analysis_errors": 0,
                          "cache_hits": 0,
                          "models_available": len(self.models)
                      }
                  
                  def get_db_connection(self):
                      """Get database connection"""
                      try:
                          config = {
                              "host": os.getenv("MYSQL_HOST", "host.docker.internal"),
                              "user": os.getenv("MYSQL_USER", "news_collector"),
                              "password": os.getenv("MYSQL_PASSWORD", "99Rules!"),
                              "database": os.getenv("MYSQL_DATABASE", "crypto_prices"),
                              "charset": "utf8mb4",
                              "autocommit": False
                          }
                          return mysql.connector.connect(**config)
                      except Exception as e:
                          logger.error(f"Database connection error: {e}")
                          return None
                  
                  def get_cached_sentiment(self, text_hash: str) -> Optional[float]:
                      """Get cached sentiment score"""
                      if not self.redis_client:
                          return None
                      
                      try:
                          cached = self.redis_client.get(f"sentiment:{text_hash}")
                          if cached:
                              SENTIMENT_CACHE_HITS.labels(source="cache").inc()
                              self.stats["cache_hits"] += 1
                              return float(cached)
                      except Exception as e:
                          logger.warning(f"Redis cache read failed: {e}")
                      
                      return None
                  
                  def cache_sentiment(self, text_hash: str, score: float, ttl_hours: int = 24):
                      """Cache sentiment score"""
                      if not self.redis_client:
                          return
                      
                      try:
                          self.redis_client.setex(f"sentiment:{text_hash}", ttl_hours * 3600, str(score))
                      except Exception as e:
                          logger.warning(f"Redis cache write failed: {e}")
                  
                  def create_text_hash(self, text: str) -> str:
                      """Create hash for text deduplication"""
                      import hashlib
                      return hashlib.md5(text.encode()).hexdigest()
                  
                  def analyze_with_textblob(self, text: str) -> Tuple[float, float]:
                      """Analyze sentiment using TextBlob"""
                      if not TEXTBLOB_AVAILABLE:
                          return 0.0, 0.0
                      
                      try:
                          blob = TextBlob(text)
                          polarity = blob.sentiment.polarity  # -1 to 1
                          subjectivity = blob.sentiment.subjectivity  # 0 to 1
                          
                          # Convert to confidence (higher subjectivity = lower confidence)
                          confidence = 1.0 - subjectivity
                          
                          return polarity, confidence
                      except Exception as e:
                          logger.error(f"TextBlob analysis error: {e}")
                          return 0.0, 0.0
                  
                  def analyze_with_vader_fallback(self, text: str) -> Tuple[float, float]:
                      """Fallback sentiment analysis using simple rules"""
                      # Simple rule-based sentiment analysis
                      positive_words = [
                          "bullish", "moon", "pump", "surge", "rally", "breakout", "adoption",
                          "partnership", "upgrade", "innovation", "growth", "profit", "gain",
                          "positive", "optimistic", "confident", "strong", "excellent"
                      ]
                      
                      negative_words = [
                          "bearish", "crash", "dump", "plunge", "decline", "correction", "fud",
                          "scam", "hack", "regulation", "ban", "negative", "pessimistic",
                          "weak", "poor", "terrible", "awful", "disaster", "loss"
                      ]
                      
                      text_lower = text.lower()
                      
                      positive_count = sum(1 for word in positive_words if word in text_lower)
                      negative_count = sum(1 for word in negative_words if word in text_lower)
                      
                      total_words = len(text.split())
                      if total_words == 0:
                          return 0.0, 0.0
                      
                      # Calculate sentiment score
                      sentiment_score = (positive_count - negative_count) / max(total_words, 1)
                      sentiment_score = max(-1.0, min(1.0, sentiment_score * 10))  # Scale and clamp
                      
                      # Calculate confidence based on word count and ratio
                      confidence = min(0.8, (positive_count + negative_count) / max(total_words, 1) * 2)
                      
                      return sentiment_score, confidence
                  
                  def analyze_with_rules(self, text: str) -> Tuple[float, float]:
                      """Rule-based sentiment analysis for crypto-specific terms"""
                      crypto_positive = [
                          "hodl", "diamond hands", "to the moon", "bull run", "alt season",
                          "defi", "nft", "metaverse", "web3", "dao", "yield farming",
                          "staking", "governance", "utility", "adoption", "institutional"
                      ]
                      
                      crypto_negative = [
                          "rug pull", "ponzi", "bubble", "crash", "bear market", "fud",
                          "whale dump", "manipulation", "scam", "hack", "exploit",
                          "regulation", "ban", "tax", "sell-off", "correction"
                      ]
                      
                      text_lower = text.lower()
                      
                      positive_count = sum(1 for phrase in crypto_positive if phrase in text_lower)
                      negative_count = sum(1 for phrase in crypto_negative if phrase in text_lower)
                      
                      if positive_count + negative_count == 0:
                          return 0.0, 0.0
                      
                      sentiment_score = (positive_count - negative_count) / (positive_count + negative_count)
                      confidence = min(0.9, (positive_count + negative_count) / 10.0)
                      
                      return sentiment_score, confidence
                  
                  def analyze_sentiment(self, text: str, source: str = "unknown") -> SentimentResult:
                      """Analyze sentiment using multiple models and return consensus"""
                      text_hash = self.create_text_hash(text)
                      
                      # Check cache first
                      cached_score = self.get_cached_sentiment(text_hash)
                      if cached_score is not None:
                          return SentimentResult(
                              text=text,
                              sentiment_score=cached_score,
                              confidence=0.8,  # Assume high confidence for cached results
                              model_used="cache",
                              source=source,
                              processed_at=datetime.now()
                          )
                      
                      # Analyze with available models
                      results = []
                      
                      for model_name, model_func in self.models.items():
                          try:
                              with SENTIMENT_ANALYSIS_DURATION.labels(model=model_name).time():
                                  score, confidence = model_func(text)
                                  
                              if score != 0.0 or confidence > 0.0:  # Only use non-zero results
                                  results.append((score, confidence, model_name))
                                  SENTIMENT_ANALYSIS_TOTAL.labels(source=source, model=model_name).inc()
                                  
                          except Exception as e:
                              logger.error(f"Error in {model_name} analysis: {e}")
                              SENTIMENT_ERRORS.labels(source=source, error_type=model_name).inc()
                              continue
                      
                      if not results:
                          # Fallback to neutral sentiment
                          final_score = 0.0
                          final_confidence = 0.0
                          model_used = "fallback"
                      else:
                          # Calculate weighted average
                          total_weight = sum(conf for _, conf, _ in results)
                          if total_weight > 0:
                              final_score = sum(score * conf for score, conf, _ in results) / total_weight
                              final_confidence = min(0.9, total_weight / len(results))
                          else:
                              final_score = sum(score for score, _, _ in results) / len(results)
                              final_confidence = 0.5
                          
                          model_used = "ensemble"
                      
                      # Cache the result
                      self.cache_sentiment(text_hash, final_score)
                      
                      # Update metrics
                      SENTIMENT_SCORE_DISTRIBUTION.labels(source=source, model=model_used).observe(final_score)
                      
                      return SentimentResult(
                          text=text,
                          sentiment_score=final_score,
                          confidence=final_confidence,
                          model_used=model_used,
                          source=source,
                          processed_at=datetime.now()
                      )
                  
                  def get_news_for_analysis(self, limit: int = 100) -> List[Dict]:
                      """Get news items that need sentiment analysis"""
                      conn = self.get_db_connection()
                      if not conn:
                          return []
                      
                      cursor = conn.cursor(dictionary=True)
                      
                      try:
                          # Get news items with no sentiment score or old sentiment
                          query = """
                          SELECT id, title, content, source, url, published_at
                          FROM crypto_news 
                          WHERE (sentiment_score = 0 OR sentiment_score IS NULL)
                          AND published_at >= %s
                          ORDER BY published_at DESC
                          LIMIT %s
                          """
                          
                          # Only analyze news from last 7 days
                          cutoff_date = datetime.now() - timedelta(days=7)
                          cursor.execute(query, (cutoff_date, limit))
                          
                          return cursor.fetchall()
                          
                      except Exception as e:
                          logger.error(f"Error fetching news for analysis: {e}")
                          return []
                      finally:
                          cursor.close()
                          conn.close()
                  
                  def update_news_sentiment(self, news_id: int, sentiment_score: float, confidence: float):
                      """Update news item with sentiment analysis"""
                      conn = self.get_db_connection()
                      if not conn:
                          return False
                      
                      cursor = conn.cursor()
                      
                      try:
                          update_query = """
                          UPDATE crypto_news 
                          SET sentiment_score = %s, 
                              sentiment_confidence = %s,
                              sentiment_updated_at = CURRENT_TIMESTAMP
                          WHERE id = %s
                          """
                          
                          cursor.execute(update_query, (sentiment_score, confidence, news_id))
                          conn.commit()
                          
                          return cursor.rowcount > 0
                          
                      except Exception as e:
                          logger.error(f"Error updating news sentiment: {e}")
                          conn.rollback()
                          return False
                      finally:
                          cursor.close()
                          conn.close()
                  
                  def run_sentiment_analysis_cycle(self) -> Dict:
                      """Run one complete sentiment analysis cycle"""
                      logger.info("Starting sentiment analysis cycle...")
                      start_time = time.time()
                      
                      # Get news items for analysis
                      news_items = self.get_news_for_analysis(limit=50)
                      
                      if not news_items:
                          logger.info("No news items found for sentiment analysis")
                          return {
                              "status": "completed",
                              "items_analyzed": 0,
                              "items_updated": 0,
                              "duration_seconds": time.time() - start_time
                          }
                      
                      analyzed_count = 0
                      updated_count = 0
                      
                      for item in news_items:
                          try:
                              # Combine title and content for analysis
                              text = f"{item['title']} {item.get('content', '')}"
                              
                              # Analyze sentiment
                              result = self.analyze_sentiment(text, source=item['source'])
                              
                              # Update database
                              if self.update_news_sentiment(
                                  item['id'], 
                                  result.sentiment_score, 
                                  result.confidence
                              ):
                                  updated_count += 1
                                  SENTIMENT_SOURCES_PROCESSED.labels(source=item['source']).inc()
                              
                              analyzed_count += 1
                              self.stats["total_analyzed"] += 1
                              
                              # Small delay to prevent overwhelming the system
                              time.sleep(0.1)
                              
                          except Exception as e:
                              logger.error(f"Error analyzing news item {item['id']}: {e}")
                              SENTIMENT_ERRORS.labels(source=item.get('source', 'unknown'), error_type="analysis").inc()
                              self.stats["analysis_errors"] += 1
                              continue
                      
                      duration = time.time() - start_time
                      
                      # Update stats
                      self.stats["total_updated"] += updated_count
                      self.stats["last_analysis"] = datetime.now()
                      
                      result = {
                          "status": "completed",
                          "items_analyzed": analyzed_count,
                          "items_updated": updated_count,
                          "duration_seconds": duration,
                          "cache_hits": self.stats["cache_hits"]
                      }
                      
                      logger.info(f"Sentiment analysis cycle completed: {result}")
                      return result
                  
                  def setup_routes(self):
                      """Setup FastAPI routes"""
                      
                      @self.app.get("/health")
                      def health():
                          """Health check endpoint"""
                          try:
                              # Check database connection
                              conn = self.get_db_connection()
                              db_status = "healthy" if conn else "unhealthy"
                              if conn:
                                  conn.close()
                              
                              # Check Redis connection
                              redis_status = "healthy"
                              if self.redis_client:
                                  try:
                                      self.redis_client.ping()
                                  except:
                                      redis_status = "unhealthy"
                              else:
                                  redis_status = "disabled"
                              
                              return {
                                  "status": "healthy" if db_status == "healthy" else "unhealthy",
                                  "service": "sentiment-collector",
                                  "database": db_status,
                                  "redis": redis_status,
                                  "models_available": self.stats["models_available"],
                                  "last_analysis": self.stats["last_analysis"]
                              }
                          except Exception as e:
                              return {
                                  "status": "unhealthy",
                                  "error": str(e)
                              }
                      
                      @self.app.get("/status")
                      def status():
                          """Status endpoint with detailed information"""
                          return {
                              "service": "sentiment-collector",
                              "version": "2.0.0",
                              "stats": self.stats,
                              "models": list(self.models.keys()),
                              "textblob_available": TEXTBLOB_AVAILABLE
                          }
                      
                      @self.app.post("/analyze")
                      def analyze_sentiment_endpoint(background_tasks: BackgroundTasks):
                          """Trigger sentiment analysis"""
                          background_tasks.add_task(self.run_sentiment_analysis_cycle)
                          return {
                              "status": "started",
                              "message": "Sentiment analysis initiated"
                          }
                      
                      @self.app.post("/analyze-text")
                      def analyze_text(text: str, source: str = "api"):
                          """Analyze sentiment for provided text"""
                          try:
                              result = self.analyze_sentiment(text, source)
                              return {
                                  "text": text,
                                  "sentiment_score": result.sentiment_score,
                                  "confidence": result.confidence,
                                  "model_used": result.model_used,
                                  "source": result.source
                              }
                          except Exception as e:
                              raise HTTPException(status_code=500, detail=str(e))
                      
                      @self.app.get("/metrics")
                      def metrics():
                          """Prometheus metrics endpoint"""
                          return Response(
                              generate_latest(),
                              media_type=CONTENT_TYPE_LATEST
                          )

              # Create the service instance
              sentiment_collector = SentimentCollector()
              app = sentiment_collector.app

              if __name__ == "__main__":
                  logger.info("ðŸš€ Starting Production Sentiment Collector Service v2.0.0")
                  uvicorn.run(app, host="0.0.0.0", port=8000)
              EOF

              # Start the service
              cd /app && python sentiment_collector.py
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: MYSQL_HOST
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: MYSQL_HOST
            - name: MYSQL_PORT
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: MYSQL_PORT
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  name: centralized-db-secrets
                  key: mysql-user
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: centralized-db-secrets
                  key: mysql-password
            - name: MYSQL_DATABASE
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: MYSQL_DATABASE
            - name: REDIS_HOST
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: REDIS_HOST
            - name: REDIS_PORT
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: REDIS_PORT
          resources:
            requests:
              cpu: 200m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: sentiment-collector
  namespace: crypto-data-collection
  labels:
    app: sentiment-collector
    component: data-collector
spec:
  selector:
    app: sentiment-collector
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: ClusterIP
