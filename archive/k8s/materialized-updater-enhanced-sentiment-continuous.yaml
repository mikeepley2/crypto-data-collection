apiVersion: v1
kind: ConfigMap
metadata:
  name: materialized-updater-code
  namespace: crypto-data-collection
data:
  materialized_updater.py: |
    import os
    import mysql.connector
    from mysql.connector import pooling
    import logging
    from datetime import datetime, timedelta
    import time
    import threading

    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    logger = logging.getLogger("materialized-updater")

    # Global connection pool
    db_pool = None

    def init_connection_pool():
        """Initialize database connection pool"""
        global db_pool
        try:
            db_pool = pooling.MySQLConnectionPool(
                pool_name="materialized_pool",
                pool_size=3,  # Small pool for materialized updater
                pool_reset_session=True,
                host=os.getenv("DB_HOST", "127.0.0.1"),
                user=os.getenv("DB_USER", "news_collector"),
                password=os.getenv("DB_PASSWORD", "99Rules!"),
                database=os.getenv("DB_NAME", "crypto_prices"),
                autocommit=False,  # We'll handle commits manually
                charset="utf8mb4"
            )
            logger.info("‚úÖ Database connection pool initialized")
            return True
        except Exception as e:
            logger.error(f"‚ùå Database pool initialization failed: {e}")
            return False

    def get_db_connection():
        """Get database connection from pool"""
        try:
            if db_pool is None:
                logger.error("Database pool not initialized")
                return None
            return db_pool.get_connection()
        except Exception as e:
            logger.error(f"Failed to get database connection: {e}")
            return None

    def retry_on_deadlock(func, max_retries=3, delay=1):
        """Retry function on deadlock with exponential backoff"""
        for attempt in range(max_retries):
            try:
                return func()
            except mysql.connector.Error as e:
                if e.errno == 1213:  # Deadlock
                    if attempt < max_retries - 1:
                        wait_time = delay * (2**attempt)
                        logger.warning(f"Deadlock detected, retrying in {wait_time}s (attempt {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"Max retries exceeded for deadlock: {e}")
                        raise
                else:
                    raise
            except Exception as e:
                logger.error(f"Unexpected error: {e}")
                raise

    def update_single_record(cursor, symbol, timestamp_iso):
        """Update a single record with retry logic"""
        def _update():
            # Time-based sentiment lookup with decay weights
            cursor.execute(
                """
                SELECT 
                    AVG(CASE 
                        WHEN published_at >= DATE_SUB(%s, INTERVAL 1 HOUR) THEN ml_sentiment_score * 1.0
                        WHEN published_at >= DATE_SUB(%s, INTERVAL 6 HOUR) THEN ml_sentiment_score * 0.8
                        WHEN published_at >= DATE_SUB(%s, INTERVAL 24 HOUR) THEN ml_sentiment_score * 0.6
                        ELSE ml_sentiment_score * 0.3
                    END) as weighted_sentiment,
                    COUNT(*) as sentiment_count,
                    AVG(ml_sentiment_score) as raw_sentiment
                FROM crypto_news
                WHERE published_at >= DATE_SUB(%s, INTERVAL 24 HOUR)
                AND published_at <= %s
                AND ml_sentiment_score IS NOT NULL
            """,
                (
                    timestamp_iso,
                    timestamp_iso,
                    timestamp_iso,
                    timestamp_iso,
                    timestamp_iso,
                ),
            )

            sentiment_data = cursor.fetchone()

            if sentiment_data and sentiment_data[0] is not None:
                weighted_sentiment, sentiment_count, raw_sentiment = sentiment_data

                # Use weighted sentiment if available, otherwise raw sentiment
                final_sentiment = (
                    weighted_sentiment if weighted_sentiment is not None else raw_sentiment
                )

                # Update the materialized record
                cursor.execute(
                    """
                    UPDATE ml_features_materialized
                    SET avg_ml_overall_sentiment = %s,
                        sentiment_volume = %s,
                        updated_at = NOW()
                    WHERE symbol = %s AND timestamp_iso = %s
                """,
                    (final_sentiment, sentiment_count, symbol, timestamp_iso),
                )

                return cursor.rowcount > 0
            return False

        return retry_on_deadlock(_update)

    def update_materialized_table():
        """Update materialized table with enhanced sentiment processing"""
        logger.info("üöÄ Starting enhanced materialized table update...")
        
        conn = get_db_connection()
        if not conn:
            return
        
        try:
            cursor = conn.cursor()
            
            # Get recent price data (last 1 hour)
            cursor.execute('''
                SELECT symbol, timestamp_iso, current_price, price_change_24h, volume_usd_24h, market_cap
                FROM price_data_real
                WHERE timestamp_iso >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
                ORDER BY timestamp_iso DESC
                LIMIT 1000
            ''')
            
            recent_data = cursor.fetchall()
            logger.info(f"Found {len(recent_data)} recent price records")
            
            inserted_count = 0
            updated_count = 0
            skipped_count = 0
            
            for row in recent_data:
                symbol, timestamp_iso, current_price, price_change_24h, volume_usd_24h, market_cap = row
                
                try:
                    # Extract date and hour from timestamp
                    price_date = timestamp_iso.date()
                    price_hour = timestamp_iso.hour
                    
                    # Enhanced sentiment data with time-based decay
                    cursor.execute('''
                        SELECT 
                            AVG(CASE 
                                WHEN published_at >= DATE_SUB(%s, INTERVAL 1 HOUR) THEN ml_sentiment_score * 1.0
                                WHEN published_at >= DATE_SUB(%s, INTERVAL 6 HOUR) THEN ml_sentiment_score * 0.8
                                WHEN published_at >= DATE_SUB(%s, INTERVAL 24 HOUR) THEN ml_sentiment_score * 0.6
                                ELSE ml_sentiment_score * 0.3
                            END) as weighted_sentiment,
                            COUNT(*) as sentiment_count,
                            AVG(ml_sentiment_score) as raw_sentiment
                        FROM crypto_news
                        WHERE published_at >= DATE_SUB(%s, INTERVAL 24 HOUR)
                        AND published_at <= %s
                        AND ml_sentiment_score IS NOT NULL
                    ''', (timestamp_iso, timestamp_iso, timestamp_iso, timestamp_iso, timestamp_iso))
                    
                    sentiment_data = cursor.fetchone()
                    if sentiment_data:
                        avg_sentiment, sentiment_count, raw_sentiment = sentiment_data
                        # Use weighted sentiment if available, otherwise raw sentiment
                        if avg_sentiment is None and raw_sentiment is not None:
                            avg_sentiment = raw_sentiment
                    else:
                        avg_sentiment, sentiment_count = None, 0
                    
                    # Get onchain data with forward-fill strategy
                    normalized_symbol = symbol.replace('-USD', '')
                    onchain_data = None
                    
                    # Try current day first
                    cursor.execute('''
                        SELECT active_addresses_24h, transaction_count_24h, exchange_net_flow_24h, price_volatility_7d
                        FROM crypto_onchain_data
                        WHERE coin_symbol COLLATE utf8mb4_unicode_ci = %s
                        AND DATE(collection_date) = DATE(%s)
                        AND active_addresses_24h IS NOT NULL
                        ORDER BY collection_date DESC
                        LIMIT 1
                    ''', (normalized_symbol, timestamp_iso))
                    onchain_data = cursor.fetchone()
                    
                    # If no current day data, try previous day
                    if not onchain_data:
                        cursor.execute('''
                            SELECT active_addresses_24h, transaction_count_24h, exchange_net_flow_24h, price_volatility_7d
                            FROM crypto_onchain_data
                            WHERE coin_symbol COLLATE utf8mb4_unicode_ci = %s
                            AND DATE(collection_date) = DATE(DATE_SUB(%s, INTERVAL 1 DAY))
                            AND active_addresses_24h IS NOT NULL
                            ORDER BY collection_date DESC
                            LIMIT 1
                        ''', (normalized_symbol, timestamp_iso))
                        onchain_data = cursor.fetchone()
                    
                    if onchain_data:
                        active_addresses_24h, transaction_count_24h, exchange_net_flow_24h, price_volatility_7d = onchain_data
                    else:
                        active_addresses_24h, transaction_count_24h, exchange_net_flow_24h, price_volatility_7d = None, None, None, None
                    
                    # Use INSERT ... ON DUPLICATE KEY UPDATE
                    insert_query = '''
                    INSERT INTO ml_features_materialized
                    (symbol, price_date, price_hour, timestamp_iso, current_price, price_change_24h, volume_24h, market_cap,
                     avg_ml_overall_sentiment, sentiment_volume,
                     active_addresses_24h, transaction_count_24h, exchange_net_flow_24h, price_volatility_7d,
                     created_at, updated_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                    ON DUPLICATE KEY UPDATE
                    current_price = VALUES(current_price),
                    price_change_24h = VALUES(price_change_24h),
                    volume_24h = VALUES(volume_24h),
                    market_cap = VALUES(market_cap),
                    avg_ml_overall_sentiment = VALUES(avg_ml_overall_sentiment),
                    sentiment_volume = VALUES(sentiment_volume),
                    active_addresses_24h = VALUES(active_addresses_24h),
                    transaction_count_24h = VALUES(transaction_count_24h),
                    exchange_net_flow_24h = VALUES(exchange_net_flow_24h),
                    price_volatility_7d = VALUES(price_volatility_7d),
                    updated_at = NOW()
                    '''
                    cursor.execute(insert_query, (symbol, price_date, price_hour, timestamp_iso, current_price, price_change_24h, volume_usd_24h, market_cap,
                                                   avg_sentiment, sentiment_count,
                                                  active_addresses_24h, transaction_count_24h, exchange_net_flow_24h, price_volatility_7d))
                    
                    if cursor.rowcount > 0:
                        if cursor.rowcount == 1:
                            inserted_count += 1
                        else:
                            updated_count += 1
                    else:
                        skipped_count += 1
                        
                except Exception as e:
                    logger.error(f'Error processing {symbol} at {timestamp_iso}: {e}')
                    skipped_count += 1
            
            conn.commit()
            logger.info(f"‚úÖ Enhanced materialized table update complete: {inserted_count} inserted, {updated_count} updated, {skipped_count} skipped")
            
            # Write health check file
            with open("/tmp/materialized_updater_health.txt", "w") as f:
                f.write(str(datetime.utcnow()))
                
        except Exception as e:
            logger.error(f"Update error: {e}")
            conn.rollback()
        finally:
            if conn:
                conn.close()  # Return connection to pool

    def continuous_sentiment_backfill():
        """Continuously backfill sentiment data in the background"""
        logger.info("üöÄ Starting continuous sentiment backfill...")
        
        conn = get_db_connection()
        if not conn:
            return
        
        try:
            cursor = conn.cursor()
            
            # Get current status
            cursor.execute("SELECT COUNT(*) FROM ml_features_materialized")
            total_records = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM ml_features_materialized WHERE avg_ml_overall_sentiment IS NOT NULL")
            current_sentiment = cursor.fetchone()[0]
            current_pct = (current_sentiment / total_records * 100) if total_records > 0 else 0
            
            logger.info(f"üìä Current Status: {current_sentiment:,}/{total_records:,} ({current_pct:.1f}%)")
            
            # Process in small batches to avoid overwhelming the system
            batch_size = 1000
            total_updated = 0
            offset = 0
            batch_count = 0
            max_batches = 20  # Process 20k records per cycle
            
            while batch_count < max_batches:
                batch_count += 1
                logger.info(f"üîÑ Processing sentiment batch {batch_count} (offset {offset})...")
                
                # Get batch of records without sentiment (most recent first)
                cursor.execute("""
                    SELECT symbol, timestamp_iso
                    FROM ml_features_materialized
                    WHERE avg_ml_overall_sentiment IS NULL
                    ORDER BY timestamp_iso DESC
                    LIMIT %s OFFSET %s
                """, (batch_size, offset))
                
                batch_records = cursor.fetchall()
                if not batch_records:
                    logger.info("No more records to process")
                    break
                
                logger.info(f"Found {len(batch_records)} records without sentiment in this batch")
                
                # Process each record individually with retry logic
                batch_updated = 0
                for symbol, timestamp_iso in batch_records:
                    try:
                        if update_single_record(cursor, symbol, timestamp_iso):
                            batch_updated += 1
                            total_updated += 1
                    except Exception as e:
                        logger.error(f"Error processing {symbol} at {timestamp_iso}: {e}")
                        continue
                
                # Commit this batch
                conn.commit()
                logger.info(f"‚úÖ Sentiment batch {batch_count} complete: {batch_updated} records updated (total: {total_updated})")
                
                offset += batch_size
                
                # Small delay between batches
                time.sleep(1)
                
                # Check progress every 5 batches
                if batch_count % 5 == 0:
                    cursor.execute("SELECT COUNT(*) FROM ml_features_materialized WHERE avg_ml_overall_sentiment IS NOT NULL")
                    current_progress = cursor.fetchone()[0]
                    progress_pct = (current_progress / total_records * 100) if total_records > 0 else 0
                    logger.info(f"üìà Sentiment Progress: {current_progress:,}/{total_records:,} ({progress_pct:.1f}%)")
            
            logger.info(f"üéâ Continuous sentiment backfill complete: {total_updated} records updated")
            
        except Exception as e:
            logger.error(f"Error in continuous sentiment backfill: {e}")
            conn.rollback()
        finally:
            if conn:
                conn.close()  # Return connection to pool

    def background_update_loop():
        """Background loop to automatically update materialized table"""
        while True:
            try:
                logger.info("üîÑ Starting automatic enhanced materialized table update...")
                update_materialized_table()
                logger.info("‚úÖ Automatic update completed")
            except Exception as e:
                logger.error(f"‚ùå Error in background update: {e}")
            
            # Wait 5 minutes before next update
            time.sleep(300)

    def background_sentiment_loop():
        """Background loop for continuous sentiment backfill"""
        while True:
            try:
                logger.info("üîÑ Starting continuous sentiment backfill...")
                continuous_sentiment_backfill()
                logger.info("‚úÖ Continuous sentiment backfill completed")
            except Exception as e:
                logger.error(f"‚ùå Error in continuous sentiment backfill: {e}")
            
            # Wait 10 minutes before next backfill cycle
            time.sleep(600)

    if __name__ == "__main__":
        logger.info("üöÄ Starting Enhanced Materialized Updater Service with Continuous Sentiment Backfill")
        
        # Initialize connection pool
        if not init_connection_pool():
            logger.error("Failed to initialize connection pool. Exiting.")
            exit(1)
        
        # Start background update thread
        update_thread = threading.Thread(target=background_update_loop, daemon=True)
        update_thread.start()
        
        # Start continuous sentiment backfill thread
        sentiment_thread = threading.Thread(target=background_sentiment_loop, daemon=True)
        sentiment_thread.start()
        
        # Keep main thread alive
        while True:
            time.sleep(60)
