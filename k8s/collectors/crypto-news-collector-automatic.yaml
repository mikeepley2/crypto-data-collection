apiVersion: apps/v1
kind: Deployment
metadata:
    name: crypto-news-collector
    namespace: crypto-data-collection
    labels:
        app: crypto-news-collector
        component: data-collection
        node-type: data-collection
spec:
    replicas: 1
    selector:
        matchLabels:
            app: crypto-news-collector
    template:
        metadata:
            labels:
                app: crypto-news-collector
                component: data-collection
                node-type: data-collection
        spec:
            nodeSelector:
                node-type: data-collection
            tolerations:
                - key: "data-platform"
                  operator: "Equal"
                  value: "true"
                  effect: "NoSchedule"
            containers:
                - name: crypto-news-collector
                  image: python:3.11-slim
                  ports:
                      - containerPort: 8000
                        name: http
                  command:
                      - /bin/bash
                      - -c
                  args:
                      - |
                          # Install dependencies
                          pip install fastapi uvicorn feedparser requests asyncio aiohttp mysql-connector-python prometheus-client tenacity circuitbreaker

                          # Create application directory
                          mkdir -p /app

                          # Create the crypto news collector service with automatic collection
                          cat > /app/main.py << 'EOF'
                          import asyncio
                          import logging
                          import os
                          import time
                          import hashlib
                          from datetime import datetime, timedelta
                          from contextlib import contextmanager
                          import mysql.connector
                          import requests
                          import aiohttp
                          import feedparser
                          from fastapi import FastAPI, BackgroundTasks
                          from prometheus_client import Counter, Histogram, Gauge, generate_latest
                          from tenacity import retry, stop_after_attempt, wait_exponential
                          from circuitbreaker import CircuitBreaker
                          import uvicorn

                          # Configure logging
                          logging.basicConfig(level=logging.INFO)
                          logger = logging.getLogger(__name__)

                          # Prometheus metrics
                          NEWS_COLLECTION_TOTAL = Counter('news_collection_requests_total', 'Total news collection requests', ['source', 'status'])
                          NEWS_COLLECTION_DURATION = Histogram('news_collection_duration_seconds', 'News collection duration', ['source'])
                          NEWS_ITEMS_STORED = Counter('news_items_stored_total', 'Total news items stored', ['source'])
                          NEWS_COLLECTION_ERRORS = Counter('news_collection_errors_total', 'News collection errors', ['source', 'error_type'])
                          ACTIVE_COLLECTION_LOOPS = Gauge('active_collection_loops', 'Number of active collection loops')
                          CIRCUIT_BREAKER_STATE = Gauge('circuit_breaker_state', 'Circuit breaker state', ['source'])

                          @contextmanager
                          def get_connection_context():
                              """Database connection context manager"""
                              conn = None
                              try:
                                  conn = mysql.connector.connect(
                                      host=os.getenv('MYSQL_HOST', 'host.docker.internal'),
                                      port=int(os.getenv('MYSQL_PORT', 3306)),
                                      user=os.getenv('MYSQL_USER', 'news_collector'),
                                      password=os.getenv('MYSQL_PASSWORD', '99Rules!'),
                                      database=os.getenv('MYSQL_DATABASE', 'crypto_prices')
                                  )
                                  yield conn
                              except Exception as e:
                                  logger.error(f"Database connection error: {e}")
                                  if conn:
                                      conn.rollback()
                                  raise
                              finally:
                                  if conn:
                                      conn.close()

                          class CryptoNewsCollector:
                              def __init__(self):
                                  self.app = FastAPI(title="Crypto News Collector Service", version="2.0.0")
                                  self.setup_routes()
                                  
                                  # Collection configuration
                                  self.collection_interval = int(os.getenv('NEWS_COLLECTION_INTERVAL', 900))  # 15 minutes
                                  self.is_collecting = False
                                  self.collection_task = None
                                  
                                  # RSS Sources
                                  self.rss_sources = [
                                      "https://www.coindesk.com/arc/outboundfeeds/rss/",
                                      "https://cointelegraph.com/rss",
                                      "https://cryptoslate.com/feed/",
                                      "https://decrypt.co/feed",
                                      "https://bitcoinmagazine.com/feed",
                                      "https://coinjournal.net/feed/",
                                      "https://www.newsbtc.com/feed/",
                                      "https://beincrypto.com/feed/",
                                      "https://www.theblock.co/rss.xml",
                                      "https://defipulse.com/blog/feed/",
                                      "https://coingecko.com/en/news/feed",
                                      "https://coinmarketcap.com/headlines/news/feed/",
                                      "https://coincenter.org/feed/",
                                      "https://www.blockchainassociation.org/feed/",
                                      "https://blog.ethereum.org/feed.xml",
                                      "https://bitcoin.org/en/rss.xml",
                                      "https://solana.com/news/rss.xml",
                                      "https://polygon.technology/blog/rss.xml",
                                      "https://cryptobriefing.com/feed/",
                                      "https://cryptodaily.co.uk/feed/",
                                      "https://ambcrypto.com/feed/",
                                      "https://cryptonews.com/news/feed/",
                                      "https://coinspeaker.com/feed/",
                                      "https://cryptoglobe.com/latest/feed/",
                                      "https://bitcoinist.com/feed/",
                                      "https://cryptonewsflash.com/feed/"
                                  ]
                                  
                                  # Circuit breakers for each source
                                  self.circuit_breakers = {
                                      source: CircuitBreaker(failure_threshold=5, recovery_timeout=60)
                                      for source in self.rss_sources
                                  }
                                  
                                  logger.info(f"ðŸš€ Crypto News Collector initialized with {self.collection_interval}s interval and {len(self.rss_sources)} RSS sources")

                              def setup_routes(self):
                                  @self.app.get("/health")
                                  async def health():
                                      return {
                                          "status": "healthy",
                                          "service": "crypto-news-collector",
                                          "version": "2.0.0",
                                          "collection_active": self.is_collecting,
                                          "collection_interval": self.collection_interval,
                                          "sources": len(self.rss_sources),
                                          "last_collection": getattr(self, 'last_collection_time', 'Never')
                                      }

                                  @self.app.get("/metrics")
                                  async def metrics():
                                      return generate_latest()

                                  @self.app.get("/status")
                                  async def status():
                                      return {
                                          "service": "crypto-news-collector",
                                          "version": "2.0.0",
                                          "collection_active": self.is_collecting,
                                          "sources": {
                                              "rss_sources": len(self.rss_sources),
                                              "api_sources": 1
                                          },
                                          "circuit_breakers": {
                                              name: "open" if cb.failure_count >= 5 else "closed"
                                              for name, cb in self.circuit_breakers.items()
                                          }
                                      }

                                  @self.app.post("/start-collection")
                                  async def start_collection(background_tasks: BackgroundTasks):
                                      if not self.is_collecting:
                                          background_tasks.add_task(self.start_background_collection)
                                          return {"status": "started", "message": "News collection started"}
                                      return {"status": "already_running", "message": "Collection already active"}

                                  @self.app.post("/stop-collection")
                                  async def stop_collection():
                                      if self.is_collecting:
                                          self.is_collecting = False
                                          if self.collection_task:
                                              self.collection_task.cancel()
                                          return {"status": "stopped", "message": "News collection stopped"}
                                      return {"status": "not_running", "message": "Collection not active"}

                                  @self.app.post("/collect")
                                  async def manual_collect(background_tasks: BackgroundTasks):
                                      background_tasks.add_task(self.collect_news_once)
                                      return {"status": "started", "message": "Manual news collection initiated"}

                              @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
                              async def collect_rss_feed(self, url):
                                  """Collect news from RSS feed with circuit breaker"""
                                  try:
                                      cb = self.circuit_breakers[url]
                                      
                                      with cb:
                                          async with aiohttp.ClientSession() as session:
                                              async with session.get(url, timeout=30) as response:
                                                  if response.status == 200:
                                                      content = await response.text()
                                                      feed = feedparser.parse(content)
                                                      
                                                      articles = []
                                                      for entry in feed.entries[:15]:  # Limit to 15 items per source
                                                          try:
                                                              # Create URL hash for duplicate detection
                                                              url_hash = hashlib.md5(entry.link.encode()).hexdigest()
                                                              
                                                              # Parse published date
                                                              published_at = datetime.now()
                                                              if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                                                  published_at = datetime(*entry.published_parsed[:6])
                                                              
                                                              articles.append({
                                                                  'title': entry.title,
                                                                  'content': getattr(entry, 'summary', ''),
                                                                  'url': entry.link,
                                                                  'published_at': published_at,
                                                                  'source': url,
                                                                  'url_hash': url_hash
                                                              })
                                                          except Exception as e:
                                                              logger.error(f"Error parsing RSS entry from {url}: {e}")
                                                              continue
                                                      
                                                      NEWS_COLLECTION_TOTAL.labels(source=url, status='success').inc()
                                                      logger.info(f"Collected {len(articles)} articles from {url}")
                                                      return articles
                                                  else:
                                                      NEWS_COLLECTION_TOTAL.labels(source=url, status='error').inc()
                                                      NEWS_COLLECTION_ERRORS.labels(source=url, error_type='http_error').inc()
                                                      logger.error(f"RSS feed error {url}: {response.status}")
                                                      return []
                                  except Exception as e:
                                      NEWS_COLLECTION_TOTAL.labels(source=url, status='error').inc()
                                      NEWS_COLLECTION_ERRORS.labels(source=url, error_type='exception').inc()
                                      logger.error(f"Error collecting RSS feed {url}: {e}")
                                      return []

                              async def collect_cryptopanic_news(self):
                                  """Collect news from CryptoPanic API"""
                                  try:
                                      url = "https://cryptopanic.com/api/v1/posts/"
                                      params = {
                                          'auth_token': 'free',  # Free tier
                                          'currencies': 'BTC,ETH,BNB,ADA,SOL,MATIC,AVAX,DOT,LINK,UNI',
                                          'filter': 'hot',
                                          'public': 'true'
                                      }
                                      
                                      async with aiohttp.ClientSession() as session:
                                          async with session.get(url, params=params, timeout=30) as response:
                                              if response.status == 200:
                                                  data = await response.json()
                                                  
                                                  articles = []
                                                  for post in data.get('results', [])[:20]:  # Limit to 20 items
                                                      try:
                                                          # Create URL hash for duplicate detection
                                                          url_hash = hashlib.md5(post['url'].encode()).hexdigest()
                                                          
                                                          # Parse published date
                                                          published_at = datetime.now()
                                                          if 'created_at' in post:
                                                              published_at = datetime.fromisoformat(post['created_at'].replace('Z', '+00:00'))
                                                          
                                                          articles.append({
                                                              'title': post['title'],
                                                              'content': post.get('body', ''),
                                                              'url': post['url'],
                                                              'published_at': published_at,
                                                              'source': 'cryptopanic',
                                                              'url_hash': url_hash
                                                          })
                                                      except Exception as e:
                                                          logger.error(f"Error parsing CryptoPanic post: {e}")
                                                          continue
                                                  
                                                  NEWS_COLLECTION_TOTAL.labels(source='cryptopanic', status='success').inc()
                                                  logger.info(f"Collected {len(articles)} articles from CryptoPanic")
                                                  return articles
                                              else:
                                                  NEWS_COLLECTION_TOTAL.labels(source='cryptopanic', status='error').inc()
                                                  NEWS_COLLECTION_ERRORS.labels(source='cryptopanic', error_type='http_error').inc()
                                                  logger.error(f"CryptoPanic API error: {response.status}")
                                                  return []
                                  except Exception as e:
                                      NEWS_COLLECTION_TOTAL.labels(source='cryptopanic', status='error').inc()
                                      NEWS_COLLECTION_ERRORS.labels(source='cryptopanic', error_type='exception').inc()
                                      logger.error(f"Error collecting CryptoPanic news: {e}")
                                      return []

                              async def store_news_articles(self, articles):
                                  """Store news articles in database"""
                                  if not articles:
                                      return 0
                                      
                                  try:
                                      with get_connection_context() as conn:
                                          cursor = conn.cursor()
                                          
                                          stored_count = 0
                                          for article in articles:
                                              try:
                                                  # Check for duplicates
                                                  cursor.execute("SELECT id FROM crypto_news WHERE url_hash = %s", (article['url_hash'],))
                                                  if cursor.fetchone():
                                                      continue  # Skip duplicate
                                                  
                                                  cursor.execute("""
                                                      INSERT INTO crypto_news (
                                                          title, content, url, published_at, source, url_hash,
                                                          category, market_type, created_at, updated_at
                                                      ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                                                  """, (
                                                      article['title'],
                                                      article['content'],
                                                      article['url'],
                                                      article['published_at'],
                                                      article['source'],
                                                      article['url_hash'],
                                                      'crypto_news',  # category
                                                      'crypto'  # market_type
                                                  ))
                                                  stored_count += 1
                                              except Exception as e:
                                                  logger.error(f"Error storing article '{article['title']}': {e}")
                                                  continue
                                          
                                          conn.commit()
                                          cursor.close()
                                          
                                          NEWS_ITEMS_STORED.labels(source=article['source']).inc(stored_count)
                                          logger.info(f"Stored {stored_count} news articles in database")
                                          return stored_count
                                  except Exception as e:
                                      logger.error(f"Error storing news articles: {e}")
                                      return 0

                              async def collect_news_once(self):
                                  """Collect news once from all sources"""
                                  logger.info("Starting news collection cycle...")
                                  start_time = time.time()
                                  
                                  try:
                                      all_articles = []
                                      
                                      # Collect from RSS feeds
                                      for rss_url in self.rss_sources:
                                          with NEWS_COLLECTION_DURATION.labels(source=rss_url).time():
                                              articles = await self.collect_rss_feed(rss_url)
                                              all_articles.extend(articles)
                                      
                                      # Collect from CryptoPanic API
                                      with NEWS_COLLECTION_DURATION.labels(source='cryptopanic').time():
                                          articles = await self.collect_cryptopanic_news()
                                          all_articles.extend(articles)
                                      
                                      # Store articles
                                      stored_count = await self.store_news_articles(all_articles)
                                      
                                      # Update last collection time
                                      self.last_collection_time = datetime.now().isoformat()
                                      
                                      duration = time.time() - start_time
                                      logger.info(f"News collection completed: {stored_count} articles from {len(all_articles)} total in {duration:.2f}s")
                                      
                                  except Exception as e:
                                      logger.error(f"Error in news collection cycle: {e}")

                              async def start_background_collection(self):
                                  """Start background news collection loop"""
                                  if self.is_collecting:
                                      logger.warning("Collection already running")
                                      return
                                  
                                  self.is_collecting = True
                                  ACTIVE_COLLECTION_LOOPS.set(1)
                                  logger.info(f"Starting background news collection every {self.collection_interval} seconds")
                                  
                                  try:
                                      while self.is_collecting:
                                          await self.collect_news_once()
                                          
                                          # Wait for next collection
                                          for _ in range(self.collection_interval):
                                              if not self.is_collecting:
                                                  break
                                              await asyncio.sleep(1)
                                  except asyncio.CancelledError:
                                      logger.info("News collection loop cancelled")
                                  except Exception as e:
                                      logger.error(f"Error in background collection loop: {e}")
                                  finally:
                                      self.is_collecting = False
                                      ACTIVE_COLLECTION_LOOPS.set(0)
                                      logger.info("Background news collection stopped")

                          # Create collector instance
                          collector = CryptoNewsCollector()
                          app = collector.app

                          if __name__ == "__main__":
                              logger.info("ðŸš€ Starting Crypto News Collector Service with Automatic Collection")
                              
                              # Start background collection automatically
                              import threading
                              def start_collection_loop():
                                  loop = asyncio.new_event_loop()
                                  asyncio.set_event_loop(loop)
                                  loop.run_until_complete(collector.start_background_collection())
                              
                              # Start collection in background thread
                              collection_thread = threading.Thread(target=start_collection_loop, daemon=True)
                              collection_thread.start()
                              
                              # Start the service
                              uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
                          EOF

                          # Start the service
                          cd /app && python main.py
                  env:
                      - name: MYSQL_HOST
                        valueFrom:
                            configMapKeyRef:
                                name: centralized-db-config
                                key: MYSQL_HOST
                      - name: MYSQL_PORT
                        valueFrom:
                            configMapKeyRef:
                                name: centralized-db-config
                                key: MYSQL_PORT
                      - name: MYSQL_USER
                        valueFrom:
                            secretKeyRef:
                                name: centralized-db-secrets
                                key: mysql-user
                      - name: MYSQL_PASSWORD
                        valueFrom:
                            secretKeyRef:
                                name: centralized-db-secrets
                                key: mysql-password
                      - name: MYSQL_DATABASE
                        valueFrom:
                            configMapKeyRef:
                                name: centralized-db-config
                                key: MYSQL_DATABASE
                      - name: NEWS_COLLECTION_INTERVAL
                        valueFrom:
                            configMapKeyRef:
                                name: centralized-db-config
                                key: NEWS_COLLECTION_INTERVAL
                  resources:
                      requests:
                          cpu: 200m
                          memory: 512Mi
                      limits:
                          cpu: 1000m
                          memory: 2Gi
                  livenessProbe:
                      httpGet:
                          path: /health
                          port: 8000
                      initialDelaySeconds: 30
                      periodSeconds: 30
                  readinessProbe:
                      httpGet:
                          path: /health
                          port: 8000
                      initialDelaySeconds: 10
                      periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
    name: crypto-news-collector
    namespace: crypto-data-collection
    labels:
        app: crypto-news-collector
        component: data-collection
spec:
    ports:
        - port: 8000
          targetPort: 8000
          protocol: TCP
          name: http
    selector:
        app: crypto-news-collector
    type: ClusterIP
