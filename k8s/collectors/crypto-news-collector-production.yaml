apiVersion: apps/v1
kind: Deployment
metadata:
  name: crypto-news-collector
  namespace: crypto-data-collection
  labels:
    app: crypto-news-collector
    component: data-collector
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/metrics"
    prometheus.io/port: "8000"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crypto-news-collector
  template:
    metadata:
      labels:
        app: crypto-news-collector
        component: data-collector
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      nodeSelector:
        node-type: data-collection # Target the data collection node
      tolerations:
        - key: "data-platform"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      containers:
        - name: crypto-news-collector
          image: python:3.11-slim
          command: ["/bin/bash", "-c"]
          args:
            - |
              # Install dependencies
              pip install fastapi uvicorn feedparser requests tenacity prometheus-client circuitbreaker redis mysql-connector-python

              # Create app directory
              mkdir -p /app

              # Create the production crypto news collector service
              cat > /app/crypto_news_collector.py << 'EOF'
              import os
              import sys
              import json
              import time
              import hashlib
              import logging
              import asyncio
              from datetime import datetime, timedelta
              from typing import List, Dict, Optional, Set
              from dataclasses import dataclass

              import feedparser
              import requests
              from requests.adapters import HTTPAdapter
              from urllib3.util.retry import Retry
              from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
              from circuitbreaker import CircuitBreaker
              from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
              import mysql.connector
              import redis
              from fastapi import FastAPI, BackgroundTasks, HTTPException
              from fastapi.responses import Response, JSONResponse
              import uvicorn

              # Set up structured logging
              logging.basicConfig(
                  level=logging.INFO,
                  format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
              )
              logger = logging.getLogger(__name__)

              # Prometheus metrics
              NEWS_COLLECTION_REQUESTS = Counter(
                  'news_collection_requests_total',
                  'Total news collection requests',
                  ['source', 'status']
              )

              NEWS_COLLECTION_DURATION = Histogram(
                  'news_collection_duration_seconds',
                  'News collection duration',
                  ['source']
              )

              NEWS_COLLECTION_ERRORS = Counter(
                  'news_collection_errors_total',
                  'Total news collection errors',
                  ['source', 'error_type']
              )

              NEWS_ITEMS_STORED = Counter(
                  'news_items_stored_total',
                  'Total news items stored',
                  ['source']
              )

              NEWS_SOURCES_ACTIVE = Gauge(
                  'news_sources_active',
                  'Number of active news sources'
              )

              CIRCUIT_BREAKER_STATE = Gauge(
                  'circuit_breaker_state',
                  'Circuit breaker state (0=closed, 1=open, 2=half-open)',
                  ['source']
              )

              @dataclass
              class NewsItem:
                  title: str
                  content: str
                  url: str
                  published_at: datetime
                  source: str
                  category: str
                  sentiment_score: float
                  crypto_mentions: List[str]
                  url_hash: str

              class NewsCollector:
                  def __init__(self):
                      self.app = FastAPI(
                          title="Crypto News Collector",
                          description="Production-grade cryptocurrency news collection service",
                          version="2.0.0"
                      )
                      self.setup_routes()
                      
                      # News sources configuration - Expanded for comprehensive coverage
                      self.rss_sources = [
                          # Major Crypto Publications
                          {
                              "name": "CoinDesk",
                              "url": "https://www.coindesk.com/arc/outboundfeeds/rss/",
                              "category": "general"
                          },
                          {
                              "name": "CoinTelegraph", 
                              "url": "https://cointelegraph.com/rss",
                              "category": "general"
                          },
                          {
                              "name": "CryptoSlate",
                              "url": "https://cryptoslate.com/feed/",
                              "category": "general"
                          },
                          {
                              "name": "Decrypt",
                              "url": "https://decrypt.co/feed",
                              "category": "general"
                          },
                          {
                              "name": "Bitcoin Magazine",
                              "url": "https://bitcoinmagazine.com/feed",
                              "category": "bitcoin"
                          },
                          {
                              "name": "CoinJournal",
                              "url": "https://coinjournal.net/feed/",
                              "category": "general"
                          },
                          {
                              "name": "NewsBTC",
                              "url": "https://www.newsbtc.com/feed/",
                              "category": "general"
                          },
                          {
                              "name": "BeInCrypto",
                              "url": "https://beincrypto.com/feed/",
                              "category": "general"
                          },
                          
                          # DeFi and Technical Focus
                          {
                              "name": "The Block",
                              "url": "https://www.theblock.co/rss.xml",
                              "category": "defi"
                          },
                          {
                              "name": "DeFi Pulse",
                              "url": "https://defipulse.com/blog/feed/",
                              "category": "defi"
                          },
                          {
                              "name": "CoinGecko",
                              "url": "https://www.coingecko.com/en/news/feed",
                              "category": "market"
                          },
                          {
                              "name": "CoinMarketCap",
                              "url": "https://coinmarketcap.com/headlines/news/feed/",
                              "category": "market"
                          },
                          
                          # Regulatory and Institutional
                          {
                              "name": "Coin Center",
                              "url": "https://www.coincenter.org/feed/",
                              "category": "regulatory"
                          },
                          {
                              "name": "Blockchain Association",
                              "url": "https://theblockchainassociation.org/feed/",
                              "category": "regulatory"
                          },
                          
                          # Technical and Development
                          {
                              "name": "Ethereum Foundation",
                              "url": "https://blog.ethereum.org/feed.xml",
                              "category": "ethereum"
                          },
                          {
                              "name": "Bitcoin Core",
                              "url": "https://bitcoin.org/en/rss.xml",
                              "category": "bitcoin"
                          },
                          {
                              "name": "Solana",
                              "url": "https://solana.com/news/rss.xml",
                              "category": "solana"
                          },
                          {
                              "name": "Polygon",
                              "url": "https://blog.polygon.technology/feed/",
                              "category": "ethereum"
                          },
                          
                          # Trading and Analysis
                          {
                              "name": "Crypto Briefing",
                              "url": "https://cryptobriefing.com/feed/",
                              "category": "analysis"
                          },
                          {
                              "name": "Crypto Daily",
                              "url": "https://cryptodaily.co.uk/feed",
                              "category": "general"
                          },
                          {
                              "name": "AMBCrypto",
                              "url": "https://ambcrypto.com/feed/",
                              "category": "analysis"
                          },
                          {
                              "name": "Crypto News",
                              "url": "https://cryptonews.com/news/feed/",
                              "category": "general"
                          },
                          
                          # International Coverage
                          {
                              "name": "CoinSpeaker",
                              "url": "https://www.coinspeaker.com/feed/",
                              "category": "international"
                          },
                          {
                              "name": "CryptoGlobe",
                              "url": "https://www.cryptoglobe.com/latest/feed/",
                              "category": "international"
                          },
                          {
                              "name": "Bitcoinist",
                              "url": "https://bitcoinist.com/feed/",
                              "category": "bitcoin"
                          },
                          {
                              "name": "Crypto News Flash",
                              "url": "https://www.crypto-news-flash.com/feed/",
                              "category": "general"
                          }
                      ]
                      
                      # API sources (free tier)
                      self.api_sources = [
                          {
                              "name": "CryptoPanic",
                              "url": "https://cryptopanic.com/api/v1/posts/",
                              "params": {"auth_token": "", "public": "true", "filter": "hot"},
                              "category": "aggregated"
                          }
                      ]
                      
                      # Circuit breakers for each source
                      self.circuit_breakers = {}
                      for source in self.rss_sources + self.api_sources:
                          self.circuit_breakers[source["name"]] = CircuitBreaker(
                              failure_threshold=5,
                              recovery_timeout=60,
                              expected_exception=Exception
                          )
                      
                      # HTTP session with retry strategy
                      self.session = requests.Session()
                      retry_strategy = Retry(
                          total=3,
                          backoff_factor=1,
                          status_forcelist=[429, 500, 502, 503, 504],
                      )
                      adapter = HTTPAdapter(max_retries=retry_strategy)
                      self.session.mount("http://", adapter)
                      self.session.mount("https://", adapter)
                      
                      # Redis connection for caching
                      try:
                          self.redis_client = redis.Redis(
                              host=os.getenv("REDIS_HOST", "redis-data-collection.crypto-data-collection.svc.cluster.local"),
                              port=int(os.getenv("REDIS_PORT", "6379")),
                              decode_responses=True
                          )
                          self.redis_client.ping()
                          logger.info("✅ Connected to Redis for caching")
                      except Exception as e:
                          logger.warning(f"⚠️ Redis connection failed: {e}")
                          self.redis_client = None
                      
                      # Stats
                      self.stats = {
                          "total_collected": 0,
                          "total_stored": 0,
                          "last_collection": None,
                          "collection_errors": 0,
                          "sources_active": len(self.rss_sources) + len(self.api_sources),
                          "duplicates_filtered": 0
                      }
                      
                      # Update metrics
                      NEWS_SOURCES_ACTIVE.set(self.stats["sources_active"])
                  
                  def get_db_connection(self):
                      """Get database connection"""
                      try:
                          config = {
                              "host": os.getenv("MYSQL_HOST", "host.docker.internal"),
                              "user": os.getenv("MYSQL_USER", "news_collector"),
                              "password": os.getenv("MYSQL_PASSWORD", "99Rules!"),
                              "database": os.getenv("MYSQL_DATABASE", "crypto_prices"),
                              "charset": "utf8mb4",
                              "autocommit": False
                          }
                          return mysql.connector.connect(**config)
                      except Exception as e:
                          logger.error(f"Database connection error: {e}")
                          return None
                  
                  def extract_crypto_mentions(self, text: str) -> List[str]:
                      """Extract cryptocurrency mentions from text"""
                      crypto_symbols = [
                          "BTC", "ETH", "BNB", "ADA", "SOL", "XRP", "DOT", "DOGE", "AVAX", "SHIB",
                          "MATIC", "LTC", "UNI", "LINK", "ATOM", "XLM", "BCH", "ALGO", "VET", "ICP",
                          "FIL", "TRX", "ETC", "XMR", "EOS", "AAVE", "SUSHI", "COMP", "MKR", "YFI"
                      ]
                      
                      mentions = []
                      text_upper = text.upper()
                      for symbol in crypto_symbols:
                          if symbol in text_upper:
                              mentions.append(symbol)
                      
                      return list(set(mentions))
                  
                  def create_url_hash(self, url: str) -> str:
                      """Create hash for URL deduplication"""
                      return hashlib.md5(url.encode()).hexdigest()
                  
                  def is_duplicate(self, url_hash: str) -> bool:
                      """Check if news item is duplicate using Redis cache"""
                      if not self.redis_client:
                          return False
                      
                      try:
                          return self.redis_client.exists(f"news_hash:{url_hash}")
                      except Exception as e:
                          logger.warning(f"Redis duplicate check failed: {e}")
                          return False
                  
                  def mark_as_processed(self, url_hash: str, ttl_hours: int = 24):
                      """Mark news item as processed in Redis"""
                      if not self.redis_client:
                          return
                      
                      try:
                          self.redis_client.setex(f"news_hash:{url_hash}", ttl_hours * 3600, "1")
                      except Exception as e:
                          logger.warning(f"Redis mark processed failed: {e}")
                  
                  @retry(
                      stop=stop_after_attempt(3),
                      wait=wait_exponential(multiplier=1, min=2, max=8),
                      retry=retry_if_exception_type((requests.RequestException,))
                  )
                  def collect_rss_news(self, source: Dict) -> List[NewsItem]:
                      """Collect news from RSS feed"""
                      circuit_breaker = self.circuit_breakers[source["name"]]
                      
                      @circuit_breaker
                      def _collect():
                          logger.info(f"Collecting RSS news from {source['name']}")
                          
                          response = self.session.get(source["url"], timeout=30)
                          response.raise_for_status()
                          
                          feed = feedparser.parse(response.content)
                          news_items = []
                          
                          for entry in feed.entries[:20]:  # Limit to 20 most recent
                              try:
                                  # Parse published date
                                  if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                      published_at = datetime(*entry.published_parsed[:6])
                                  else:
                                      published_at = datetime.now()
                                  
                                  # Create URL hash for deduplication
                                  url_hash = self.create_url_hash(entry.link)
                                  
                                  # Check for duplicates
                                  if self.is_duplicate(url_hash):
                                      self.stats["duplicates_filtered"] += 1
                                      continue
                                  
                                  # Extract content
                                  content = ""
                                  if hasattr(entry, 'summary'):
                                      content = entry.summary
                                  elif hasattr(entry, 'description'):
                                      content = entry.description
                                  
                                  # Extract crypto mentions
                                  crypto_mentions = self.extract_crypto_mentions(
                                      f"{entry.title} {content}"
                                  )
                                  
                                  news_item = NewsItem(
                                      title=entry.title,
                                      content=content,
                                      url=entry.link,
                                      published_at=published_at,
                                      source=source["name"],
                                      category=source["category"],
                                      sentiment_score=0.0,  # Will be calculated by sentiment service
                                      crypto_mentions=crypto_mentions,
                                      url_hash=url_hash
                                  )
                                  
                                  news_items.append(news_item)
                                  self.mark_as_processed(url_hash)
                                  
                              except Exception as e:
                                  logger.error(f"Error parsing RSS entry from {source['name']}: {e}")
                                  continue
                          
                          logger.info(f"Collected {len(news_items)} news items from {source['name']}")
                          return news_items
                      
                      try:
                          return _collect()
                      except Exception as e:
                          logger.error(f"Circuit breaker open for {source['name']}: {e}")
                          CIRCUIT_BREAKER_STATE.labels(source=source["name"]).set(1)  # Open
                          NEWS_COLLECTION_ERRORS.labels(source=source["name"], error_type="circuit_breaker").inc()
                          return []
                  
                  @retry(
                      stop=stop_after_attempt(3),
                      wait=wait_exponential(multiplier=1, min=2, max=8),
                      retry=retry_if_exception_type((requests.RequestException,))
                  )
                  def collect_api_news(self, source: Dict) -> List[NewsItem]:
                      """Collect news from API source"""
                      circuit_breaker = self.circuit_breakers[source["name"]]
                      
                      @circuit_breaker
                      def _collect():
                          logger.info(f"Collecting API news from {source['name']}")
                          
                          response = self.session.get(source["url"], params=source["params"], timeout=30)
                          response.raise_for_status()
                          
                          data = response.json()
                          news_items = []
                          
                          if source["name"] == "CryptoPanic":
                              for post in data.get("results", [])[:20]:  # Limit to 20
                                  try:
                                      # Parse published date
                                      published_at = datetime.fromisoformat(
                                          post["published_at"].replace("Z", "+00:00")
                                      )
                                      
                                      # Create URL hash
                                      url_hash = self.create_url_hash(post["url"])
                                      
                                      # Check for duplicates
                                      if self.is_duplicate(url_hash):
                                          self.stats["duplicates_filtered"] += 1
                                          continue
                                      
                                      # Extract crypto mentions
                                      crypto_mentions = self.extract_crypto_mentions(
                                          f"{post['title']} {post.get('body', '')}"
                                      )
                                      
                                      news_item = NewsItem(
                                          title=post["title"],
                                          content=post.get("body", ""),
                                          url=post["url"],
                                          published_at=published_at,
                                          source=source["name"],
                                          category=source["category"],
                                          sentiment_score=0.0,
                                          crypto_mentions=crypto_mentions,
                                          url_hash=url_hash
                                      )
                                      
                                      news_items.append(news_item)
                                      self.mark_as_processed(url_hash)
                                      
                                  except Exception as e:
                                      logger.error(f"Error parsing API entry from {source['name']}: {e}")
                                      continue
                          
                          logger.info(f"Collected {len(news_items)} news items from {source['name']}")
                          return news_items
                      
                      try:
                          return _collect()
                      except Exception as e:
                          logger.error(f"Circuit breaker open for {source['name']}: {e}")
                          CIRCUIT_BREAKER_STATE.labels(source=source["name"]).set(1)  # Open
                          NEWS_COLLECTION_ERRORS.labels(source=source["name"], error_type="circuit_breaker").inc()
                          return []
                  
                  def store_news_items(self, news_items: List[NewsItem]) -> int:
                      """Store news items in database"""
                      if not news_items:
                          return 0
                      
                      conn = self.get_db_connection()
                      if not conn:
                          logger.error("No database connection available")
                          return 0
                      
                      cursor = conn.cursor()
                      stored_count = 0
                      
                      try:
                          for item in news_items:
                              try:
                                  insert_sql = """
                                  INSERT INTO crypto_news (
                                      title, content, url, published_at, source, 
                                      category, sentiment_score, crypto_mentions, url_hash
                                  ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                                  ON DUPLICATE KEY UPDATE
                                      content = VALUES(content),
                                      sentiment_score = VALUES(sentiment_score),
                                      updated_at = CURRENT_TIMESTAMP
                                  """
                                  
                                  cursor.execute(insert_sql, (
                                      item.title,
                                      item.content,
                                      item.url,
                                      item.published_at,
                                      item.source,
                                      item.category,
                                      item.sentiment_score,
                                      ",".join(item.crypto_mentions),
                                      item.url_hash
                                  ))
                                  
                                  stored_count += 1
                                  NEWS_ITEMS_STORED.labels(source=item.source).inc()
                                  
                              except Exception as e:
                                  logger.error(f"Error storing news item: {e}")
                                  continue
                          
                          conn.commit()
                          logger.info(f"Stored {stored_count} news items in database")
                          
                      except Exception as e:
                          logger.error(f"Database transaction error: {e}")
                          conn.rollback()
                      finally:
                          cursor.close()
                          conn.close()
                      
                      return stored_count
                  
                  def run_collection_cycle(self) -> Dict:
                      """Run one complete news collection cycle"""
                      logger.info("Starting news collection cycle...")
                      start_time = time.time()
                      
                      total_collected = 0
                      total_stored = 0
                      
                      # Collect from RSS sources
                      for source in self.rss_sources:
                          try:
                              with NEWS_COLLECTION_DURATION.labels(source=source["name"]).time():
                                  news_items = self.collect_rss_news(source)
                                  
                              if news_items:
                                  stored = self.store_news_items(news_items)
                                  total_collected += len(news_items)
                                  total_stored += stored
                                  NEWS_COLLECTION_REQUESTS.labels(source=source["name"], status="success").inc()
                              else:
                                  NEWS_COLLECTION_REQUESTS.labels(source=source["name"], status="empty").inc()
                              
                              # Small delay between sources
                              time.sleep(1)
                              
                          except Exception as e:
                              logger.error(f"Error processing RSS source {source['name']}: {e}")
                              NEWS_COLLECTION_ERRORS.labels(source=source["name"], error_type="collection").inc()
                              NEWS_COLLECTION_REQUESTS.labels(source=source["name"], status="error").inc()
                              continue
                      
                      # Collect from API sources
                      for source in self.api_sources:
                          try:
                              with NEWS_COLLECTION_DURATION.labels(source=source["name"]).time():
                                  news_items = self.collect_api_news(source)
                                  
                              if news_items:
                                  stored = self.store_news_items(news_items)
                                  total_collected += len(news_items)
                                  total_stored += stored
                                  NEWS_COLLECTION_REQUESTS.labels(source=source["name"], status="success").inc()
                              else:
                                  NEWS_COLLECTION_REQUESTS.labels(source=source["name"], status="empty").inc()
                              
                              # Delay between API calls
                              time.sleep(2)
                              
                          except Exception as e:
                              logger.error(f"Error processing API source {source['name']}: {e}")
                              NEWS_COLLECTION_ERRORS.labels(source=source["name"], error_type="collection").inc()
                              NEWS_COLLECTION_REQUESTS.labels(source=source["name"], status="error").inc()
                              continue
                      
                      duration = time.time() - start_time
                      
                      # Update stats
                      self.stats["total_collected"] += total_collected
                      self.stats["total_stored"] += total_stored
                      self.stats["last_collection"] = datetime.now()
                      
                      result = {
                          "status": "completed",
                          "duration_seconds": duration,
                          "items_collected": total_collected,
                          "items_stored": total_stored,
                          "sources_processed": len(self.rss_sources) + len(self.api_sources),
                          "duplicates_filtered": self.stats["duplicates_filtered"],
                          "errors": self.stats["collection_errors"]
                      }
                      
                      logger.info(f"Collection cycle completed: {result}")
                      return result
                  
                  def setup_routes(self):
                      """Setup FastAPI routes"""
                      
                      @self.app.get("/health")
                      def health():
                          """Health check endpoint"""
                          try:
                              # Check database connection
                              conn = self.get_db_connection()
                              db_status = "healthy" if conn else "unhealthy"
                              if conn:
                                  conn.close()
                              
                              # Check Redis connection
                              redis_status = "healthy"
                              if self.redis_client:
                                  try:
                                      self.redis_client.ping()
                                  except:
                                      redis_status = "unhealthy"
                              else:
                                  redis_status = "disabled"
                              
                              return {
                                  "status": "healthy" if db_status == "healthy" else "unhealthy",
                                  "service": "crypto-news-collector",
                                  "database": db_status,
                                  "redis": redis_status,
                                  "last_collection": self.stats["last_collection"],
                                  "sources_active": self.stats["sources_active"]
                              }
                          except Exception as e:
                              return {
                                  "status": "unhealthy",
                                  "error": str(e)
                              }
                      
                      @self.app.get("/status")
                      def status():
                          """Status endpoint with detailed information"""
                          return {
                              "service": "crypto-news-collector",
                              "version": "2.0.0",
                              "stats": self.stats,
                              "sources": {
                                  "rss": len(self.rss_sources),
                                  "api": len(self.api_sources),
                                  "total": len(self.rss_sources) + len(self.api_sources)
                              },
                              "circuit_breakers": {
                                  name: "open" if cb.failure_count >= 5 else "closed"
                                  for name, cb in self.circuit_breakers.items()
                              }
                          }
                      
                      @self.app.post("/collect")
                      def collect_news(background_tasks: BackgroundTasks):
                          """Trigger news collection"""
                          background_tasks.add_task(self.run_collection_cycle)
                          return {
                              "status": "started",
                              "message": "News collection initiated",
                              "sources": len(self.rss_sources) + len(self.api_sources)
                          }
                      
                      @self.app.get("/metrics")
                      def metrics():
                          """Prometheus metrics endpoint"""
                          return Response(
                              generate_latest(),
                              media_type=CONTENT_TYPE_LATEST
                          )

              # Create the service instance
              news_collector = NewsCollector()
              app = news_collector.app

              if __name__ == "__main__":
                  logger.info("🚀 Starting Production Crypto News Collector Service v2.0.0")
                  uvicorn.run(app, host="0.0.0.0", port=8000)
              EOF

              # Start the service
              cd /app && python crypto_news_collector.py
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: MYSQL_HOST
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: MYSQL_HOST
            - name: MYSQL_PORT
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: MYSQL_PORT
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  name: centralized-db-secrets
                  key: mysql-user
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: centralized-db-secrets
                  key: mysql-password
            - name: MYSQL_DATABASE
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: MYSQL_DATABASE
            - name: REDIS_HOST
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: REDIS_HOST
            - name: REDIS_PORT
              valueFrom:
                configMapKeyRef:
                  name: centralized-db-config
                  key: REDIS_PORT
          resources:
            requests:
              cpu: 300m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 1Gi
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: crypto-news-collector
  namespace: crypto-data-collection
  labels:
    app: crypto-news-collector
    component: data-collector
spec:
  selector:
    app: crypto-news-collector
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: ClusterIP
