#!/usr/bin/env python3
"""
Enhanced Onchain Data Collector v2.0
Production-ready onchain metrics collector following OHLC template patterns
Includes FastAPI health endpoints, comprehensive metrics tracking, and health scoring

Features:
- Health scoring system (0-100)
- FastAPI endpoints for monitoring
- Comprehensive statistics tracking
- Multiple data sources with fallbacks
- Rate limiting and error handling
- Automatic backfill capabilities
"""

import os
import logging
import time
import asyncio
import json
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional, Tuple
import mysql.connector
from mysql.connector.errors import Error as MySQLError
import aiohttp
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
import sys

# Import centralized database configuration and symbol normalization
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from shared.database_config import get_db_connection, test_db_connection, get_db_config
from shared.symbol_normalizer import StandardSymbolNormalizer

# Configure logging with UTF-8 encoding support
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s:%(name)s:%(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/tmp/onchain_collector.log', mode='a', encoding='utf-8')
    ]
)

# Set console encoding to UTF-8 for Windows compatibility
if sys.stdout.encoding != 'utf-8':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    import io
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
logger = logging.getLogger("enhanced-onchain-collector")

class OnchainDataSource:
    """Configuration for an onchain data source"""
    def __init__(self, source_id: str, url_template: str, rate_limit: float, 
                 priority: int, parse_func: str):
        self.source_id = source_id
        self.url_template = url_template
        self.rate_limit = rate_limit
        self.priority = priority
        self.parse_func = parse_func
        self.last_call = 0
        self.success_count = 0
        self.error_count = 0

class EnhancedOnchainCollectorV2:
    """Enhanced Onchain Collector v2 with health scoring and FastAPI endpoints"""
    
    def __init__(self):
        self.app = FastAPI(title="Enhanced Onchain Collector", version="2.0.0")
        self.setup_routes()
        
        # Database configuration - Use centralized config with connection pool optimization
        try:
            self.db_config = get_db_config()
            if not test_db_connection():
                raise ConnectionError("Database connection test failed")
            logger.info(f"[DB] Using centralized database config: {self.db_config['host']}:{self.db_config['port']}")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize centralized database config: {e}")
            # Fallback to environment variables
            self.db_config = {
                "host": os.getenv("MYSQL_HOST", "172.22.32.1"),
                "user": os.getenv("MYSQL_USER", "news_collector"),
                "password": os.getenv("MYSQL_PASSWORD", "99Rules!"),
                "database": os.getenv("MYSQL_DATABASE", "crypto_prices"),
                "charset": "utf8mb4",
                "autocommit": True,
            }
        
        # API configurations
        self.coingecko_api_key = os.getenv('COINGECKO_API_KEY', 'CG-94NCcVD2euxaGTZe94bS2oYz')
        
        # Symbol mapping cache for efficient lookups (standardized approach)
        self.symbol_normalizer = StandardSymbolNormalizer()
        
        # Data sources configuration - CoinGecko only (comprehensive onchain data)
        self.data_sources = {
            'coingecko': OnchainDataSource(
                'coingecko', 
                'https://api.coingecko.com/api/v3/coins/{symbol}',
                0.1 if self.coingecko_api_key.startswith('CG-') else 1.0,
                1, 'parse_coingecko_data'
            )
        }
        
        # Statistics tracking
        self.statistics = {
            'total_collections': 0,
            'successful_collections': 0,
            'failed_collections': 0,
            'records_collected': 0,
            'api_calls_made': 0,
            'last_collection': None,
            'last_success': None,
            'last_error': None,
            'database_writes': 0,
            'symbols_processed': 0
        }
        
        # Health metrics
        self.health_metrics = {
            'consecutive_failures': 0,
            'api_error_count': 0,
            'database_error_count': 0,
            'last_health_check': datetime.now()
        }
        
        # Supported symbols with enhanced metadata
        self.supported_symbols = [
            'BTC', 'ETH', 'ADA', 'SOL', 'AVAX', 'DOT', 'MATIC', 'LINK',
            'UNI', 'ATOM', 'ALGO', 'XTZ', 'NEAR', 'FTM', 'ONE', 'VET'
        ]
        
        self.collection_interval = int(os.getenv("COLLECTION_INTERVAL", "3600"))  # 1 hour default
        
        logger.info("[INIT] Enhanced Onchain Collector v2.0 initialized")
        logger.info(f"[API] CoinGecko API: {'Premium' if self.coingecko_api_key.startswith('CG-') else 'Free'}")
        logger.info(f"[SYMBOLS] Supporting {len(self.supported_symbols)} symbols")

    def setup_routes(self):
        """Setup FastAPI routes"""
        
        @self.app.get("/health")
        async def health_check():
            """Health check endpoint"""
            health_score = self.calculate_health_score()
            
            # Update last health check
            self.health_metrics['last_health_check'] = datetime.now()
            
            # Determine status
            status = "healthy" if health_score >= 80 else "degraded" if health_score >= 60 else "unhealthy"
            
            return JSONResponse({
                "status": status,
                "health_score": health_score,
                "timestamp": datetime.now().isoformat(),
                "data_freshness": self.get_data_freshness()
            })
        
        @self.app.get("/status")
        async def get_status():
            """Detailed status endpoint"""
            gap_days = self.get_data_gap_days()
            logger.info(f"üìä Gap analysis: {gap_days} days since last data")
            
            return {
                "service": "enhanced-onchain-collector",
                "version": "2.0.0",
                "statistics": self.statistics.copy(),
                "health_metrics": self.health_metrics.copy(),
                "symbols": {
                    "total": len(self.supported_symbols),
                    "active": len(self.supported_symbols)
                },
                "data_sources": {
                    source_id: {
                        "success_count": source.success_count,
                        "error_count": source.error_count,
                        "rate_limit": source.rate_limit,
                        "priority": source.priority
                    } for source_id, source in self.data_sources.items()
                },
                "data_freshness": {
                    "gap_days": gap_days,
                    "status": "healthy" if gap_days <= 2 else "stale"
                },
                "health_score": self.calculate_health_score(),
                "collection_interval": self.collection_interval,
                "timestamp": datetime.now().isoformat()
            }
        
        @self.app.post("/collect")
        async def manual_collection():
            """Manual collection trigger"""
            try:
                logger.info("üì° Manual collection triggered")
                result = await self.collect_all_symbols()
                return {
                    "status": "completed",
                    "result": result,
                    "timestamp": datetime.now().isoformat()
                }
            except Exception as e:
                logger.error(f"Manual collection failed: {e}")
                raise HTTPException(status_code=500, detail=f"Collection failed: {str(e)}")
        
        @self.app.post("/backfill")
        async def manual_backfill():
            """Manual backfill trigger"""
            try:
                logger.info("üì° Manual backfill triggered")
                end_date = date.today() - timedelta(days=1)
                start_date = end_date - timedelta(days=30)  # Last 30 days
                result = await self.run_comprehensive_backfill(start_date, end_date)
                return {
                    "status": "completed",
                    "result": result,
                    "timestamp": datetime.now().isoformat()
                }
            except Exception as e:
                logger.error(f"Manual backfill failed: {e}")
                raise HTTPException(status_code=500, detail=f"Backfill failed: {str(e)}")

    def calculate_health_score(self) -> float:
        """Calculate health score (0-100) based on multiple factors"""
        score = 100.0
        
        # Consecutive failures penalty (max -40 points)
        consecutive_failures = self.health_metrics['consecutive_failures']
        if consecutive_failures > 0:
            score -= min(consecutive_failures * 10, 40)
        
        # API error rate penalty (max -30 points)
        total_api_calls = sum(source.success_count + source.error_count for source in self.data_sources.values())
        if total_api_calls > 0:
            api_error_rate = self.health_metrics['api_error_count'] / total_api_calls
            score -= min(api_error_rate * 30, 30)
        
        # Database error penalty (max -30 points)
        db_error_count = self.health_metrics['database_error_count']
        if db_error_count > 0:
            score -= min(db_error_count, 30)
        
        # Data freshness bonus/penalty
        gap_days = self.get_data_gap_days()
        if gap_days <= 1:
            pass  # No penalty for fresh data
        elif gap_days <= 3:
            score -= 10  # Minor penalty
        else:
            score -= 20  # Significant penalty for stale data
        
        return max(0.0, min(100.0, score))

    def get_data_gap_days(self) -> int:
        """Get number of days since last successful data collection"""
        try:
            conn = mysql.connector.connect(**self.db_config)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT MAX(DATE(timestamp_iso)) as last_date
                FROM onchain_data 
                WHERE data_source LIKE '%enhanced-onchain-collector%'
            """)
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if result[0]:
                last_date = result[0]
                gap = (date.today() - last_date).days
                return gap
            else:
                return 999  # No data found
                
        except Exception as e:
            logger.warning(f"Error calculating data gap: {e}")
            return 999

    def get_data_freshness(self) -> Dict:
        """Get data freshness information"""
        gap_days = self.get_data_gap_days()
        
        if gap_days <= 1:
            status = "fresh"
        elif gap_days <= 3:
            status = "acceptable"
        else:
            status = "stale"
        
        return {
            "gap_days": gap_days,
            "status": status,
            "last_update": self.statistics.get('last_success')
        }

    def get_db_connection(self):
        """Get database connection with error handling"""
        try:
            return mysql.connector.connect(**self.db_config)
        except MySQLError as e:
            self.health_metrics['database_error_count'] += 1
            logger.error(f"Database connection error: {e}")
            raise

    def ensure_onchain_table(self):
        """Ensure onchain_data table exists with proper schema"""
        try:
            conn = get_db_connection()  # Use centralized connection
            cursor = conn.cursor()
            
            # Enhanced onchain_data table with all required fields
            create_table_sql = """
                CREATE TABLE IF NOT EXISTS onchain_data (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    symbol VARCHAR(100) NOT NULL,
                    coin_id VARCHAR(150),
                    timestamp_iso DATETIME(6) NOT NULL,
                    
                    -- Network metrics
                    active_addresses BIGINT,
                    transaction_count BIGINT,
                    transaction_volume DECIMAL(25,8),
                    hash_rate DECIMAL(25,8),
                    difficulty DECIMAL(25,8),
                    block_height BIGINT,
                    block_time_seconds DECIMAL(10,2),
                    
                    -- Supply metrics  
                    circulating_supply DECIMAL(25,8),
                    total_supply DECIMAL(25,8),
                    max_supply DECIMAL(25,8),
                    supply_inflation_rate DECIMAL(10,4),
                    
                    -- Market metrics
                    market_cap_rank INT,
                    fully_diluted_valuation DECIMAL(25,2),
                    market_cap_change_24h DECIMAL(25,2),
                    market_cap_change_percentage_24h DECIMAL(10,4),
                    
                    -- Price performance
                    ath DECIMAL(25,8),
                    ath_change_percentage DECIMAL(10,4),
                    atl DECIMAL(25,8),
                    atl_change_percentage DECIMAL(10,4),
                    high_24h DECIMAL(25,8),
                    low_24h DECIMAL(25,8),
                    price_change_24h DECIMAL(25,8),
                    price_change_percentage_24h DECIMAL(10,4),
                    price_change_percentage_7d DECIMAL(10,4),
                    price_change_percentage_30d DECIMAL(10,4),
                    price_change_percentage_1y DECIMAL(10,4),
                    
                    -- Volume and liquidity
                    total_volume DECIMAL(25,2),
                    
                    -- Network value metrics
                    network_value_to_transactions DECIMAL(20,8),
                    realized_cap DECIMAL(25,2),
                    mvrv_ratio DECIMAL(10,4),
                    nvt_ratio DECIMAL(10,4),
                    
                    -- Development metrics
                    github_commits_30d INT,
                    github_forks INT,
                    github_stars INT,
                    github_subscribers INT,
                    github_total_issues INT,
                    github_closed_issues INT,
                    github_pull_requests_merged INT,
                    github_pull_request_contributors INT,
                    developer_activity_score DECIMAL(10,4),
                    
                    -- Staking metrics (PoS networks)
                    staking_yield DECIMAL(10,4),
                    staked_percentage DECIMAL(10,4),
                    validator_count INT,
                    
                    -- DeFi metrics (smart contract platforms)
                    total_value_locked DECIMAL(25,2),
                    defi_protocols_count INT,
                    
                    -- Data quality and source
                    data_source VARCHAR(200),
                    data_quality_score DECIMAL(3,2) DEFAULT 1.0,
                    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    
                    UNIQUE KEY unique_symbol_timestamp (symbol, timestamp_iso),
                    INDEX idx_symbol (symbol),
                    INDEX idx_timestamp (timestamp_iso),
                    INDEX idx_collected (collected_at),
                    INDEX idx_data_source (data_source)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            
            cursor.execute(create_table_sql)
            conn.commit()
            cursor.close()
            conn.close()
            
            logger.info("‚úÖ onchain_data table schema verified")
            
        except Exception as e:
            logger.error(f"Error ensuring onchain_data table: {e}")
            self.health_metrics['database_error_count'] += 1
            raise

    def get_active_symbols(self) -> List[str]:
        """Get active symbols using standardized symbol normalizer"""
        try:
            # Use standardized symbol normalizer for consistent symbol handling
            symbols = self.symbol_normalizer.get_active_symbols(
                coinbase_only=True, 
                limit=20  # Limit for onchain data collection performance
            )
            
            logger.info(f"[SYMBOLS] Retrieved {len(symbols)} active symbols via standardized normalizer")
            return symbols
            
        except Exception as e:
            logger.warning(f"Error fetching symbols from standardized normalizer: {e}")
        
        # Fallback to supported symbols
        logger.info(f"[SYMBOLS] Using fallback symbols: {len(self.supported_symbols)} symbols")
        return self.supported_symbols

    async def rate_limit_source(self, source_id: str):
        """Apply rate limiting for a specific data source"""
        source = self.data_sources[source_id]
        now = time.time()
        
        if source.last_call > 0:
            time_since_last = now - source.last_call
            if time_since_last < source.rate_limit:
                sleep_time = source.rate_limit - time_since_last
                await asyncio.sleep(sleep_time)
        
        source.last_call = time.time()

    async def fetch_coingecko_data(self, session: aiohttp.ClientSession, symbol: str, target_date: Optional[date] = None) -> Optional[Dict]:
        """Fetch comprehensive data from CoinGecko API - current or historical"""
        try:
            await self.rate_limit_source('coingecko')
            self.statistics['api_calls_made'] += 1
            
            # Get CoinGecko ID mapping
            coin_id = self.get_coingecko_id_mapping(symbol)
            
            # Premium vs free endpoint selection
            if self.coingecko_api_key.startswith('CG-'):
                base_url = "https://pro-api.coingecko.com/api/v3"
                headers = {'x-cg-pro-api-key': self.coingecko_api_key}
                logger.debug(f"üî• Using CoinGecko Pro API for full historical access")
            else:
                base_url = "https://api.coingecko.com/api/v3"
                headers = {}
                logger.debug(f"‚ö†Ô∏è Using CoinGecko Free API (365-day historical limitation)")
            
            # Use historical endpoint for backfill, current endpoint for real-time
            if target_date:
                # Pro API has full historical access, Free API has 365-day limitation
                if self.coingecko_api_key.startswith('CG-'):
                    # Pro API: Full historical access
                    date_str = target_date.strftime('%d-%m-%Y')
                    url = f"{base_url}/coins/{coin_id}/history"
                    params = {'date': date_str, 'localization': 'false'}
                    logger.debug(f"üî• [{date_str}] {symbol}: Using Pro API historical endpoint")
                else:
                    # Free API: Check 365-day limitation
                    today = date.today()
                    days_ago = (today - target_date).days
                    
                    if days_ago <= 365:
                        date_str = target_date.strftime('%d-%m-%Y')
                        url = f"{base_url}/coins/{coin_id}/history"
                        params = {'date': date_str, 'localization': 'false'}
                        logger.debug(f"üìÖ [{date_str}] {symbol}: Using Free API historical endpoint ({days_ago} days ago)")
                    else:
                        logger.warning(f"‚ö†Ô∏è [{target_date}] {symbol}: Date is {days_ago} days ago (> 365), using current data")
                        url = f"{base_url}/coins/{coin_id}"
                        params = {
                            'localization': 'false',
                            'tickers': 'false',
                            'market_data': 'true',
                            'community_data': 'true',
                            'developer_data': 'true',
                            'sparkline': 'false'
                        }
            else:
                # Current data endpoint
                url = f"{base_url}/coins/{coin_id}"
                params = {
                    'localization': 'false',
                    'tickers': 'false',
                    'market_data': 'true',
                    'community_data': 'true',
                    'developer_data': 'true',
                    'sparkline': 'false'
                }
                logger.debug(f"üìä {symbol}: Using current data endpoint")
            
            async with session.get(url, headers=headers, params=params, timeout=30) as response:
                if response.status == 200:
                    data = await response.json()
                    self.data_sources['coingecko'].success_count += 1
                    return self.parse_coingecko_data(data, symbol, target_date)
                elif response.status == 429:
                    logger.warning(f"üö´ CoinGecko rate limited for {symbol}")
                    await asyncio.sleep(5)
                    self.data_sources['coingecko'].error_count += 1
                    self.health_metrics['api_error_count'] += 1
                    return None
                else:
                    logger.warning(f"‚ö†Ô∏è CoinGecko API error for {symbol}: {response.status}")
                    self.data_sources['coingecko'].error_count += 1
                    self.health_metrics['api_error_count'] += 1
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Error fetching CoinGecko data for {symbol}: {e}")
            self.data_sources['coingecko'].error_count += 1
            self.health_metrics['api_error_count'] += 1
            return None

    def get_coingecko_id_mapping(self, symbol: str) -> str:
        """Get CoinGecko ID using standardized symbol normalizer"""
        try:
            return self.symbol_normalizer.get_coingecko_id(symbol)
        except Exception as e:
            logger.debug(f"Error getting CoinGecko ID for {symbol}: {e}")
            return symbol.lower()

    def parse_coingecko_data(self, data: Dict, symbol: str, target_date: Optional[date] = None) -> Dict:
        """Parse CoinGecko response into onchain metrics (handles both current and historical data)"""
        try:
            # Historical vs current data have different structures
            if target_date:
                # Historical data format
                market_data = data.get('market_data', {})
                date_str = target_date.strftime('%Y-%m-%d')
                logger.debug(f"üìÖ [{date_str}] {symbol}: Parsing historical data")
            else:
                # Current data format  
                market_data = data.get('market_data', {})
                logger.debug(f"üìä {symbol}: Parsing current data")
                
            developer_data = data.get('developer_data', {}) or {}
            
            # Validate we have market data
            if not market_data:
                date_str = target_date.strftime('%Y-%m-%d') if target_date else 'current'
                logger.warning(f"‚ö†Ô∏è [{date_str}] {symbol}: No market data found in CoinGecko response")
                return None
                
            # Extract USD values (work for both current and historical)
            market_cap_usd = self.extract_usd_value(market_data.get('market_cap'))
            total_volume_usd = self.extract_usd_value(market_data.get('total_volume'))
            fdv_usd = self.extract_usd_value(market_data.get('fully_diluted_valuation'))
            current_price = self.extract_usd_value(market_data.get('current_price'))
            
            # Calculate network value metrics
            nvt_ratio = None
            if market_cap_usd and total_volume_usd and total_volume_usd > 0:
                nvt_ratio = market_cap_usd / total_volume_usd
            
            mvrv_ratio = None
            if market_cap_usd and fdv_usd and fdv_usd > 0:
                mvrv_ratio = market_cap_usd / fdv_usd
            
            parsed_data = {
                'symbol': symbol,
                'coin_id': data.get('id'),
                'timestamp_iso': datetime.now(),
                
                # Supply metrics
                'circulating_supply': market_data.get('circulating_supply'),
                'total_supply': market_data.get('total_supply'),
                'max_supply': market_data.get('max_supply'),
                
                # Market metrics
                'market_cap_rank': market_data.get('market_cap_rank'),
                'fully_diluted_valuation': fdv_usd,
                'market_cap_change_24h': market_data.get('market_cap_change_24h'),
                'market_cap_change_percentage_24h': market_data.get('market_cap_change_percentage_24h'),
                
                # Price performance
                'ath': self.extract_usd_value(market_data.get('ath')),
                'ath_change_percentage': market_data.get('ath_change_percentage'),
                'atl': self.extract_usd_value(market_data.get('atl')),
                'atl_change_percentage': market_data.get('atl_change_percentage'),
                'high_24h': self.extract_usd_value(market_data.get('high_24h')),
                'low_24h': self.extract_usd_value(market_data.get('low_24h')),
                'price_change_24h': market_data.get('price_change_24h'),
                'price_change_percentage_24h': market_data.get('price_change_percentage_24h'),
                'price_change_percentage_7d': market_data.get('price_change_percentage_7d'),
                'price_change_percentage_30d': market_data.get('price_change_percentage_30d'),
                'price_change_percentage_1y': market_data.get('price_change_percentage_1y'),
                
                # Volume and liquidity
                'total_volume': total_volume_usd,
                
                # Network value metrics
                'nvt_ratio': nvt_ratio,
                'mvrv_ratio': mvrv_ratio,
                
                # Development metrics
                'github_commits_30d': developer_data.get('commit_count_4_weeks'),
                'github_forks': developer_data.get('forks'),
                'github_stars': developer_data.get('stars'),
                'github_subscribers': developer_data.get('subscribers'),
                'github_total_issues': developer_data.get('total_issues'),
                'github_closed_issues': developer_data.get('closed_issues'),
                'github_pull_requests_merged': developer_data.get('pull_requests_merged'),
                'github_pull_request_contributors': developer_data.get('pull_request_contributors'),
                'developer_activity_score': self.calculate_dev_score(developer_data),
                
                'data_source': 'enhanced-onchain-collector-v2-coingecko',
                'data_quality_score': 0.95 if self.coingecko_api_key.startswith('CG-') else 0.85
            }
            
            # Add blockchain-specific data
            blockchain_data = self.get_blockchain_specific_metrics(symbol, market_data)
            parsed_data.update(blockchain_data)
            
            # Sanitize all data for database storage
            sanitized_data = self.sanitize_data_for_storage(parsed_data)
            
            # Add pricing metadata for logging (not stored in database)
            sanitized_data['_logging_metadata'] = {
                'price_usd': current_price,
                'market_cap_usd': market_cap_usd,
                'total_volume_usd': total_volume_usd
            }
            
            return sanitized_data
            
        except Exception as e:
            logger.error(f"Error parsing CoinGecko data for {symbol}: {e}")
            return None

    def extract_usd_value(self, value_dict) -> Optional[float]:
        """Extract USD value from CoinGecko currency dict"""
        if isinstance(value_dict, dict) and 'usd' in value_dict:
            return value_dict['usd']
        elif isinstance(value_dict, (int, float)):
            return float(value_dict)
        return None

    def sanitize_data_for_storage(self, data: Dict) -> Dict:
        """Sanitize data to ensure all values are scalar (not dict/list) for database storage"""
        sanitized = {}
        
        # Define the fields that actually exist in the current table schema
        valid_fields = {
            'symbol', 'coin_id', 'timestamp_iso',
            'circulating_supply', 'total_supply', 'max_supply',
            'mvrv_ratio', 'nvt_ratio',
            'github_commits_30d', 'developer_activity_score',
            'staking_yield', 'staked_percentage', 'validator_count',
            'total_value_locked', 'defi_protocols_count',
            'data_source', 'data_quality_score'
        }
        
        for key, value in data.items():
            # Only include fields that exist in the current table schema  
            if key not in valid_fields:
                continue
                
            if isinstance(value, dict):
                # Extract USD value if it's a currency dict, otherwise skip
                if 'usd' in value:
                    sanitized[key] = value['usd']
                else:
                    sanitized[key] = None
            elif isinstance(value, list):
                # Convert lists to None or first element
                sanitized[key] = value[0] if value else None
            elif isinstance(value, (str, int, float, bool)) or value is None:
                sanitized[key] = value
            else:
                # For any other types, convert to string or None
                try:
                    sanitized[key] = str(value) if value is not None else None
                except:
                    sanitized[key] = None
        
        # Ensure all required fields have default values if missing
        defaults = {
            'staking_yield': None,
            'staked_percentage': None, 
            'validator_count': None,
            'total_value_locked': None,
            'defi_protocols_count': None,
            'developer_activity_score': None
        }
        
        for field, default in defaults.items():
            if field not in sanitized:
                sanitized[field] = default
                
        return sanitized

    def calculate_dev_score(self, dev_data: Dict) -> Optional[float]:
        """Calculate normalized developer activity score"""
        try:
            commits = dev_data.get('commit_count_4_weeks', 0) or 0
            if commits > 0:
                # Normalize to 0-1 scale (100 commits = 1.0)
                return min(commits / 100.0, 1.0)
        except:
            pass
        return None

    def get_blockchain_specific_metrics(self, symbol: str, market_data: Dict) -> Dict:
        """Get blockchain-specific metrics and estimates"""
        metrics = {}
        
        # Staking data for PoS networks
        staking_data = {
            'ADA': {'staking_yield': 4.5, 'staked_percentage': 72, 'validator_count': 3000},
            'DOT': {'staking_yield': 12.0, 'staked_percentage': 55, 'validator_count': 297},
            'SOL': {'staking_yield': 6.8, 'staked_percentage': 75, 'validator_count': 1400},
            'AVAX': {'staking_yield': 9.2, 'staked_percentage': 65, 'validator_count': 1200},
            'MATIC': {'staking_yield': 3.8, 'staked_percentage': 40, 'validator_count': 100},
            'ETH': {'staking_yield': 3.2, 'staked_percentage': 22, 'validator_count': 900000},
            'ATOM': {'staking_yield': 19.0, 'staked_percentage': 68, 'validator_count': 175},
            'ALGO': {'staking_yield': 5.8, 'staked_percentage': 60, 'validator_count': 120},
            'XTZ': {'staking_yield': 6.0, 'staked_percentage': 80, 'validator_count': 450},
            'NEAR': {'staking_yield': 10.5, 'staked_percentage': 45, 'validator_count': 100}
        }
        
        if symbol in staking_data:
            metrics.update(staking_data[symbol])
        
        # DeFi TVL data for smart contract platforms
        tvl_data = {
            'ETH': {'total_value_locked': 50000000000, 'defi_protocols_count': 150},
            'SOL': {'total_value_locked': 3000000000, 'defi_protocols_count': 40},
            'AVAX': {'total_value_locked': 2000000000, 'defi_protocols_count': 50},
            'MATIC': {'total_value_locked': 1000000000, 'defi_protocols_count': 70},
            'FTM': {'total_value_locked': 800000000, 'defi_protocols_count': 30},
            'NEAR': {'total_value_locked': 400000000, 'defi_protocols_count': 12}
        }
        
        if symbol in tvl_data:
            metrics.update(tvl_data[symbol])
        
        # Network activity estimates
        market_cap = self.extract_usd_value(market_data.get('market_cap', {}))
        if market_cap:
            network_estimates = self.estimate_network_activity(symbol, market_cap)
            metrics.update(network_estimates)
        
        return metrics

    def estimate_network_activity(self, symbol: str, market_cap: float) -> Dict:
        """Estimate network activity based on symbol and market cap"""
        estimates = {}
        
        # Network-specific multipliers for activity estimation
        activity_multipliers = {
            'BTC': {'tx_per_billion': 150, 'addr_per_billion': 800},
            'ETH': {'tx_per_billion': 500, 'addr_per_billion': 1200},
            'ADA': {'tx_per_billion': 80, 'addr_per_billion': 400},
            'SOL': {'tx_per_billion': 2000, 'addr_per_billion': 1500},
            'AVAX': {'tx_per_billion': 800, 'addr_per_billion': 1000},
            'DOT': {'tx_per_billion': 60, 'addr_per_billion': 300},
            'MATIC': {'tx_per_billion': 3500, 'addr_per_billion': 2000},
            'LINK': {'tx_per_billion': 100, 'addr_per_billion': 500},
        }
        
        multiplier = activity_multipliers.get(symbol, {'tx_per_billion': 200, 'addr_per_billion': 600})
        market_cap_billions = market_cap / 1_000_000_000
        
        estimates['transaction_count'] = int(market_cap_billions * multiplier['tx_per_billion'])
        estimates['active_addresses'] = int(market_cap_billions * multiplier['addr_per_billion'])
        
        # Block time estimates
        block_times = {
            'BTC': 600, 'ETH': 12, 'ADA': 20, 'SOL': 0.4, 'AVAX': 2,
            'DOT': 6, 'MATIC': 2.1, 'ATOM': 6.5, 'ALGO': 4.5, 'XTZ': 60
        }
        if symbol in block_times:
            estimates['block_time_seconds'] = block_times[symbol]
        
        # Transaction volume estimate (percentage of market cap)
        volume_ratios = {
            'BTC': 0.15, 'ETH': 0.25, 'SOL': 0.30, 'AVAX': 0.20,
            'MATIC': 0.35, 'ADA': 0.08, 'DOT': 0.05
        }
        ratio = volume_ratios.get(symbol, 0.10)
        estimates['transaction_volume'] = market_cap * ratio
        
        return estimates

    async def collect_symbol_data(self, symbol: str, target_date: Optional[date] = None) -> Optional[Dict]:
        """Collect comprehensive onchain data for a single symbol using CoinGecko only"""
        try:
            # Enhanced logging with date and context
            date_str = target_date.strftime('%Y-%m-%d') if target_date else 'current'
            coin_id = self.symbol_normalizer.get_coingecko_id(symbol)
            logger.info(f"üìä [{date_str}] Collecting onchain data: {symbol} (CoinGecko ID: {coin_id})")
            
            async with aiohttp.ClientSession() as session:
                # Use CoinGecko's comprehensive API (includes all onchain metrics)
                coingecko_data = await self.fetch_coingecko_data(session, symbol, target_date)
                
                if coingecko_data:
                    # Set collection timestamp
                    if target_date:
                        coingecko_data['timestamp_iso'] = datetime.combine(target_date, datetime.min.time())
                    else:
                        coingecko_data['timestamp_iso'] = datetime.now()
                    
                    # Enhanced success logging with key metrics from logging metadata
                    logging_meta = coingecko_data.get('_logging_metadata', {})
                    price = logging_meta.get('price_usd', 0) or 0
                    market_cap = logging_meta.get('market_cap_usd', 0) or 0
                    dev_score = coingecko_data.get('developer_activity_score', 0) or 0
                    supply = coingecko_data.get('circulating_supply', 0) or 0
                    
                    if target_date:
                        logger.info(f"‚úÖ [{date_str}] {symbol}: Price ${price:.4f}, Market Cap ${market_cap:,.0f}, Dev Score: {dev_score:.2f}, Supply: {supply:,.0f}, Fields: {len(coingecko_data)}")
                    else:
                        logger.info(f"‚úÖ [current] {symbol}: Price ${price:.4f}, Market Cap ${market_cap:,.0f}, Dev Score: {dev_score:.2f}, Supply: {supply:,.0f}, Fields: {len(coingecko_data)}")
                    
                    return coingecko_data
                else:
                    logger.warning(f"‚ö†Ô∏è [{date_str}] {symbol}: No data retrieved from CoinGecko")
                    
                return None
                
        except Exception as e:
            date_str = target_date.strftime('%Y-%m-%d') if target_date else 'current'
            logger.error(f"‚ùå [{date_str}] Error collecting data for {symbol}: {e}")
            return None

    def store_onchain_data(self, data_records: List[Dict]) -> int:
        """Store onchain data to database with proper connection cleanup"""
        if not data_records:
            return 0
        
        # Remove logging metadata from records before storing
        clean_records = []
        for record in data_records:
            clean_record = {k: v for k, v in record.items() if not k.startswith('_')}
            clean_records.append(clean_record)
        
        conn = None
        cursor = None
        try:
            conn = get_db_connection()  # Use centralized connection
            cursor = conn.cursor()
            
            # INSERT query matching the actual table schema (existing fields only)
            insert_query = """
                INSERT INTO onchain_data (
                    symbol, coin_id, timestamp_iso,
                    circulating_supply, total_supply, max_supply,
                    mvrv_ratio, nvt_ratio,
                    github_commits_30d, developer_activity_score,
                    staking_yield, staked_percentage, validator_count,
                    total_value_locked, defi_protocols_count,
                    data_source, data_quality_score
                ) VALUES (
                    %(symbol)s, %(coin_id)s, %(timestamp_iso)s,
                    %(circulating_supply)s, %(total_supply)s, %(max_supply)s,
                    %(mvrv_ratio)s, %(nvt_ratio)s,
                    %(github_commits_30d)s, %(developer_activity_score)s,
                    %(staking_yield)s, %(staked_percentage)s, %(validator_count)s,
                    %(total_value_locked)s, %(defi_protocols_count)s,
                    %(data_source)s, %(data_quality_score)s
                ) ON DUPLICATE KEY UPDATE
                    circulating_supply = VALUES(circulating_supply),
                    total_supply = VALUES(total_supply),
                    max_supply = VALUES(max_supply),
                    mvrv_ratio = VALUES(mvrv_ratio),
                    nvt_ratio = VALUES(nvt_ratio),
                    github_commits_30d = VALUES(github_commits_30d),
                    developer_activity_score = VALUES(developer_activity_score),
                    staking_yield = VALUES(staking_yield),
                    staked_percentage = VALUES(staked_percentage),
                    validator_count = VALUES(validator_count),
                    total_value_locked = VALUES(total_value_locked),
                    defi_protocols_count = VALUES(defi_protocols_count),
                    data_source = VALUES(data_source),
                    data_quality_score = VALUES(data_quality_score)
            """
            
            cursor.executemany(insert_query, clean_records)
            rows_affected = cursor.rowcount
            conn.commit()
            
            self.statistics['database_writes'] += rows_affected
            
            logger.info(f"[STORE] Stored {rows_affected} onchain records")
            return rows_affected
            
        except Exception as e:
            logger.error(f"Error storing onchain data: {e}")
            self.health_metrics['database_error_count'] += 1
            return 0
        finally:
            # Always cleanup connections to prevent pool exhaustion
            if cursor:
                try:
                    cursor.close()
                except Exception:
                    pass
            if conn:
                try:
                    conn.close()
                except Exception:
                    pass

    def get_missing_dates(self, symbol: str, start_date: date, end_date: date) -> List[date]:
        """Get dates missing onchain data for a symbol"""
        conn = None
        cursor = None
        try:
            conn = get_db_connection()  # Use centralized connection
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT DISTINCT DATE(timestamp_iso) as date_only
                FROM onchain_data
                WHERE symbol = %s
                AND DATE(timestamp_iso) >= %s
                AND DATE(timestamp_iso) <= %s
                AND data_source LIKE '%enhanced-onchain-collector-v2%'
            """, (symbol, start_date, end_date))
            
            existing_dates = {row[0] for row in cursor.fetchall()}
            
            # Generate all dates in range
            all_dates = []
            current = start_date
            while current <= end_date:
                all_dates.append(current)
                current += timedelta(days=1)
            
            missing = [d for d in all_dates if d not in existing_dates]
            
            return missing
            
        except Exception as e:
            logger.error(f"Error getting missing dates for {symbol}: {e}")
            return []
        finally:
            # Always cleanup connections
            if cursor:
                try:
                    cursor.close()
                except Exception:
                    pass
            if conn:
                try:
                    conn.close()
                except Exception:
                    pass

    async def collect_all_symbols(self) -> Dict:
        """Collect data for all active symbols"""
        logger.info("[START] Starting onchain data collection for all symbols")
        self.statistics['total_collections'] += 1
        
        # Ensure table exists
        self.ensure_onchain_table()
        
        symbols = self.get_active_symbols()
        collected_data = []
        failed_symbols = []
        
        for symbol in symbols:
            try:
                data = await self.collect_symbol_data(symbol)
                if data:
                    collected_data.append(data)
                    logger.info(f"‚úÖ Collected data for {symbol}")
                else:
                    failed_symbols.append(symbol)
                    logger.warning(f"‚ùå Failed to collect data for {symbol}")
                
                # Rate limiting between symbols
                await asyncio.sleep(1.0)
                
            except Exception as e:
                logger.error(f"Error processing {symbol}: {e}")
                failed_symbols.append(symbol)
        
        # Store collected data
        if collected_data:
            stored_count = self.store_onchain_data(collected_data)
            self.statistics['records_collected'] += stored_count
            self.statistics['symbols_processed'] = len(symbols)
            
            if len(failed_symbols) == 0:
                self.statistics['successful_collections'] += 1
                self.statistics['last_success'] = datetime.now().isoformat()
                self.health_metrics['consecutive_failures'] = 0
            else:
                self.statistics['failed_collections'] += 1
                self.statistics['last_error'] = f"Failed symbols: {len(failed_symbols)}"
                self.health_metrics['consecutive_failures'] += 1
        else:
            self.statistics['failed_collections'] += 1
            self.statistics['last_error'] = "No data collected for any symbol"
            self.health_metrics['consecutive_failures'] += 1
        
        result = {
            'symbols_processed': len(symbols),
            'data_collected': len(collected_data),
            'failed_symbols': failed_symbols,
            'records_stored': stored_count if collected_data else 0
        }
        
        logger.info(f"üìä Collection completed: {result}")
        return result

    async def run_comprehensive_backfill(self, start_date: date, end_date: date) -> Dict:
        """Run ultra-fast bulk backfill for historical onchain data"""
        logger.info(f"üöÄ Starting ULTRA-FAST onchain backfill from {start_date} to {end_date}")
        logger.info(f"üìä Using optimized batch processing for maximum efficiency")
        
        # Ensure table exists
        self.ensure_onchain_table()
        
        symbols = self.get_active_symbols()
        total_processed = 0
        total_stored = 0
        errors = []
        
        # Calculate date range for logging
        total_days = (end_date - start_date).days + 1
        logger.info(f"üóìÔ∏è Processing {len(symbols)} symbols across {total_days} days = {len(symbols) * total_days} total data points")
        
        async with aiohttp.ClientSession() as session:
            for symbol in symbols:
                try:
                    coin_id = self.symbol_normalizer.get_coingecko_id(symbol)
                    logger.info(f"üîç [{symbol}] Starting bulk collection (CoinGecko ID: {coin_id})")
                    
                    # Get missing dates for this symbol
                    missing_dates = self.get_missing_dates(symbol, start_date, end_date)
                    
                    if not missing_dates:
                        logger.info(f"‚úÖ [{symbol}] No missing dates - already complete!")
                        continue
                    
                    logger.info(f"üìÖ [{symbol}] Found {len(missing_dates)} missing dates to backfill")
                    
                    # Process all dates for this symbol at once (batch collection)
                    symbol_data = []
                    for target_date in missing_dates:
                        data = await self.collect_symbol_data(symbol, target_date)
                        if data:
                            symbol_data.append(data)
                            total_processed += 1
                        
                        # Small delay to avoid rate limiting
                        await asyncio.sleep(0.1)
                    
                    # Store all data for this symbol at once
                    if symbol_data:
                        stored = self.store_onchain_data(symbol_data)
                        total_stored += stored
                        logger.info(f"üíæ [{symbol}] Stored {stored} records ({len(symbol_data)} collected)")
                    else:
                        logger.warning(f"‚ö†Ô∏è [{symbol}] No data collected")
                    
                    # Progress update
                    symbols_complete = len([s for s in symbols if s == symbol or symbols.index(s) < symbols.index(symbol)])
                    progress = (symbols_complete / len(symbols)) * 100
                    logger.info(f"üìà Progress: {symbols_complete}/{len(symbols)} symbols ({progress:.1f}%)")
                    
                    # Delay between symbols to respect API limits
                    await asyncio.sleep(1.0)
                    
                except Exception as e:
                    error_msg = f"Error processing {symbol}: {e}"
                    logger.error(f"‚ùå [{symbol}] {error_msg}")
                    errors.append(error_msg)
        
        result = {
            'symbols_processed': len(symbols),
            'dates_processed': total_processed,
            'records_stored': total_stored,
            'errors': errors,
            'date_range': f"{start_date} to {end_date}",
            'total_days': total_days,
            'efficiency': f"{total_stored}/{len(symbols) * total_days}"
        }
        
        logger.info(f"üéâ ULTRA-FAST backfill completed: {result}")
        return result
    
    async def run_ultra_fast_bulk_backfill(self, start_date: date, end_date: date, target_table: str = 'crypto_prices') -> Dict:
        """Ultra-fast bulk backfill using CoinGecko range endpoints for market data"""
        logger.info(f"[BACKFILL] Starting ultra-fast bulk backfill from {start_date} to {end_date}")
        logger.info(f"[TABLE] Target table: {target_table}")
        
        if target_table == 'crypto_prices':
            return await self._bulk_market_data_backfill(start_date, end_date)
        else:
            # For onchain_data table, fall back to comprehensive backfill
            logger.info("üìä Using comprehensive backfill for onchain_data table")
            return await self.run_comprehensive_backfill(start_date, end_date)
    
    async def _bulk_market_data_backfill(self, start_date: date, end_date: date) -> Dict:
        """Bulk market data collection using CoinGecko range endpoints"""
        symbols = self.get_active_symbols()
        total_symbols = len(symbols)
        processed = 0
        errors = 0
        total_stored = 0
        
        logger.info(f"[BULK] Starting bulk market data collection for {total_symbols} symbols")
        
        # Calculate unix timestamps
        from_timestamp = int(datetime.combine(start_date, datetime.min.time()).timestamp())
        to_timestamp = int(datetime.combine(end_date, datetime.min.time()).timestamp())
        
        async with aiohttp.ClientSession() as session:
            for symbol in symbols:
                try:
                    logger.info(f"[PROCESS] Processing bulk data for {symbol}")
                    
                    # Get CoinGecko ID for symbol
                    coin_id = self.get_coingecko_id(symbol)
                    if not coin_id:
                        logger.warning(f"‚ö†Ô∏è No CoinGecko ID found for {symbol}")
                        continue
                    
                    # Use CoinGecko range endpoint for bulk historical data
                    url = f"https://pro-api.coingecko.com/api/v3/coins/{coin_id}/market_chart/range"
                    params = {
                        'vs_currency': 'usd',
                        'from': from_timestamp,
                        'to': to_timestamp,
                        'x_cg_pro_api_key': self.coingecko_api_key
                    }
                    
                    async with session.get(url, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            # Process bulk data points for price_data_real table
                            stored_count = await self._process_bulk_market_data_real(symbol, coin_id, data)
                            total_stored += stored_count
                            
                            logger.info(f"[SUCCESS] Processed {stored_count} data points for {symbol}")
                        else:
                            logger.warning(f"[WARN] Failed to fetch bulk data for {symbol}: {response.status}")
                            errors += 1
                    
                    processed += 1
                    
                    # Rate limiting
                    await asyncio.sleep(2.0)
                    
                except Exception as e:
                    logger.error(f"‚ùå Error processing bulk data for {symbol}: {e}")
                    errors += 1
        
        summary = {
            'total_symbols': total_symbols,
            'processed': processed,
            'errors': errors,
            'total_records_stored': total_stored,
            'date_range': f"{start_date} to {end_date}",
            'method': 'ultra_fast_bulk'
        }
        
        logger.info(f"[COMPLETE] Ultra-fast bulk backfill complete: {summary}")
        return summary
    
    async def _process_bulk_market_data(self, symbol: str, data: dict) -> int:
        """Process bulk market data and store to crypto_prices table"""
        stored_count = 0
        
        if not data or 'prices' not in data:
            return stored_count
        
        prices = data.get('prices', [])
        market_caps = data.get('market_caps', [])
        total_volumes = data.get('total_volumes', [])
        
        # Create lookup dictionaries for efficiency
        market_cap_dict = {int(item[0]): item[1] for item in market_caps}
        volume_dict = {int(item[0]): item[1] for item in total_volumes}
        
        # Prepare batch insert data
        batch_data = []
        
        for price_point in prices:
            try:
                timestamp_ms = int(price_point[0])
                price = float(price_point[1])
                
                # Convert timestamp to datetime
                timestamp_dt = datetime.fromtimestamp(timestamp_ms / 1000)
                timestamp_iso = timestamp_dt.isoformat()
                
                # Get corresponding market cap and volume
                market_cap = market_cap_dict.get(timestamp_ms)
                volume = volume_dict.get(timestamp_ms)
                
                # Create record matching crypto_prices schema
                record = (
                    symbol.upper(),
                    timestamp_ms,  # timestamp (bigint)
                    timestamp_dt,  # timestamp_iso (datetime)
                    price,  # price
                    market_cap,  # market_cap
                    volume,  # volume
                    price,  # high (approximation)
                    price,  # low (approximation)
                    None,   # price_change_24h (will calculate later)
                    None    # percent_change_24h (will calculate later)
                )
                
                batch_data.append(record)
                
            except Exception as e:
                logger.warning(f"[WARN] Error processing bulk data point for {symbol}: {e}")
                continue
        
        # Batch insert into crypto_prices table
        if batch_data:
            stored_count = await self._batch_store_market_data(batch_data)
        
        return stored_count
    
    async def _process_bulk_market_data_real(self, symbol: str, coin_id: str, data: dict) -> int:
        """Process bulk market data and store to price_data_real table"""
        stored_count = 0
        
        if not data or 'prices' not in data:
            return stored_count
        
        prices = data.get('prices', [])
        market_caps = data.get('market_caps', [])
        total_volumes = data.get('total_volumes', [])
        
        # Create lookup dictionaries for efficiency
        market_cap_dict = {int(item[0]): item[1] for item in market_caps}
        volume_dict = {int(item[0]): item[1] for item in total_volumes}
        
        # Prepare batch insert data
        batch_data = []
        
        for price_point in prices:
            try:
                timestamp_ms = int(price_point[0])
                price = float(price_point[1])
                
                # Convert timestamp to datetime
                timestamp_dt = datetime.fromtimestamp(timestamp_ms / 1000)
                
                # Get corresponding market cap and volume
                market_cap = market_cap_dict.get(timestamp_ms)
                volume = volume_dict.get(timestamp_ms)
                
                # Create record matching price_data_real schema
                record = (
                    symbol.upper(),      # symbol
                    coin_id,            # coin_id  
                    symbol.upper(),     # name (use symbol as fallback)
                    timestamp_ms,       # timestamp (bigint)
                    timestamp_dt,       # timestamp_iso (datetime)
                    price,              # current_price
                    None,               # price_change_24h (calculate later)
                    None,               # price_change_percentage_24h (calculate later)
                    market_cap,         # market_cap
                    volume,             # volume_usd_24h
                    None,               # volume_qty_24h (calculate later)
                    None,               # market_cap_rank (get from API later)
                    None,               # circulating_supply (get from API later)
                    None,               # total_supply (get from API later)
                    None,               # max_supply (get from API later)
                    None,               # ath (get from API later)
                    None,               # ath_date (get from API later)
                    None,               # atl (get from API later)
                    None,               # atl_date (get from API later)
                    '1d',               # collection_interval
                    'enhanced-onchain-collector-v2-bulk',  # data_source
                    1,                  # collector_container
                    int(timestamp_ms / 1000),  # collection_run
                    0.8,                # data_quality_score
                    price,              # high_24h (approximation)
                    price,              # low_24h (approximation)
                    price               # open_24h (approximation)
                )
                
                batch_data.append(record)
                
            except Exception as e:
                logger.warning(f"[WARN] Error processing bulk data point for {symbol}: {e}")
                continue
        
        # Batch insert into price_data_real table
        if batch_data:
            stored_count = await self._batch_store_price_data_real(batch_data)
        
        return stored_count

    async def _batch_store_price_data_real(self, batch_data: list) -> int:
        """Batch store market data to price_data_real table"""
        if not batch_data:
            return 0
        
        try:
            # Use synchronous connection for MySQL
            db_config = get_db_config()
            conn = mysql.connector.connect(**db_config)
            cursor = conn.cursor()
            
            # Prepare insert query matching price_data_real table schema
            insert_query = """
            INSERT INTO price_data_real (
                symbol, coin_id, name, timestamp, timestamp_iso, current_price, 
                price_change_24h, price_change_percentage_24h, market_cap, volume_usd_24h,
                volume_qty_24h, market_cap_rank, circulating_supply, total_supply, max_supply,
                ath, ath_date, atl, atl_date, collection_interval, data_source, 
                collector_container, collection_run, data_quality_score, high_24h, low_24h, open_24h,
                created_at
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW()
            ) ON DUPLICATE KEY UPDATE
                current_price = VALUES(current_price),
                market_cap = VALUES(market_cap),
                volume_usd_24h = VALUES(volume_usd_24h),
                high_24h = VALUES(high_24h),
                low_24h = VALUES(low_24h),
                open_24h = VALUES(open_24h),
                data_quality_score = VALUES(data_quality_score),
                created_at = NOW()
            """
            
            # Execute batch insert
            cursor.executemany(insert_query, batch_data)
            conn.commit()
            
            stored_count = cursor.rowcount
            
            cursor.close()
            conn.close()
            
            return stored_count
            
        except Exception as e:
            logger.error(f"[ERROR] Error batch storing price data to price_data_real: {e}")
            return 0

    async def _batch_store_market_data(self, batch_data: list) -> int:
        """Batch store market data to crypto_prices table with correct schema"""
        if not batch_data:
            return 0
        
        try:
            # Use synchronous connection for MySQL
            db_config = get_db_config()
            conn = mysql.connector.connect(**db_config)
            cursor = conn.cursor()
            
            # Prepare insert query matching actual crypto_prices table schema
            insert_query = """
            INSERT INTO crypto_prices (symbol, timestamp, timestamp_iso, price, market_cap, volume, high, low, price_change_24h, percent_change_24h, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
            ON DUPLICATE KEY UPDATE
                price = VALUES(price),
                market_cap = VALUES(market_cap),
                volume = VALUES(volume),
                high = VALUES(high),
                low = VALUES(low),
                price_change_24h = VALUES(price_change_24h),
                percent_change_24h = VALUES(percent_change_24h),
                created_at = NOW()
            """
            
            # Execute batch insert
            cursor.executemany(insert_query, batch_data)
            conn.commit()
            
            stored_count = cursor.rowcount
            
            cursor.close()
            conn.close()
            
            return stored_count
            
        except Exception as e:
            logger.error(f"[ERROR] Error batch storing market data: {e}")
            return 0
    
    def get_coingecko_id(self, symbol: str) -> str:
        """Get CoinGecko ID for a symbol using standardized symbol normalizer"""
        try:
            coingecko_id = self.symbol_normalizer.get_coingecko_id(symbol)
            logger.debug(f"[MAPPING] Standardized lookup for {symbol}: {coingecko_id}")
            return coingecko_id
        except Exception as e:
            logger.warning(f"[MAPPING] Standardized lookup failed for {symbol}: {e}")
            # Ultimate fallback to lowercase symbol
            fallback_id = symbol.lower()
            logger.warning(f"[MAPPING] Using lowercase fallback for {symbol}: {fallback_id}")
            return fallback_id
    
    def get_coin_name(self, symbol: str) -> str:
        """Get coin name for a symbol using standardized symbol normalizer"""
        try:
            coin_name = self.symbol_normalizer.get_coin_name(symbol)
            return coin_name
        except Exception as e:
            logger.warning(f"[MAPPING] Standardized name lookup failed for {symbol}: {e}")
            # Fallback to formatted symbol
            return symbol.replace('_', ' ').title()

    async def run_service(self):
        """Run the onchain collector service"""
        logger.info("[SERVICE] Starting Enhanced Onchain Collector v2 service")
        
        # Initial table setup
        self.ensure_onchain_table()
        
        while True:
            try:
                # Run regular collection
                await self.collect_all_symbols()
                
                # Wait for next collection
                logger.info(f"[WAIT] Waiting {self.collection_interval} seconds until next collection")
                await asyncio.sleep(self.collection_interval)
                
            except Exception as e:
                logger.error(f"Service error: {e}")
                self.health_metrics['consecutive_failures'] += 1
                await asyncio.sleep(60)  # Wait 1 minute before retry

# Application factory
def create_app() -> FastAPI:
    """Create and configure FastAPI application"""
    collector = EnhancedOnchainCollectorV2()
    return collector.app

# Main execution
if __name__ == "__main__":
    collector = EnhancedOnchainCollectorV2()
    
    # Check for backfill mode
    backfill_days = os.getenv("BACKFILL_DAYS")
    backfill_mode = os.getenv("BACKFILL_MODE", "comprehensive")  # 'comprehensive' or 'ultra_fast'
    target_table = os.getenv("TARGET_TABLE", "onchain_data")  # 'onchain_data' or 'crypto_prices'
    
    if backfill_days:
        async def run_backfill():
            try:
                days = int(backfill_days)
                end_date = date.today() - timedelta(days=1)
                
                if days == 0:
                    start_date = date(2023, 1, 1)
                    logger.info(f"üîÑ Running FULL historical backfill ({backfill_mode} mode, table: {target_table})")
                else:
                    start_date = end_date - timedelta(days=days)
                    logger.info(f"üîÑ Running backfill for last {days} days ({backfill_mode} mode, table: {target_table})")
                
                if backfill_mode == "ultra_fast":
                    result = await collector.run_ultra_fast_bulk_backfill(start_date, end_date, target_table)
                    print(f"Ultra-fast bulk backfill result: {result}")
                else:
                    result = await collector.run_comprehensive_backfill(start_date, end_date)
                    print(f"Comprehensive onchain backfill result: {result}")
                
            except ValueError:
                logger.error(f"Invalid BACKFILL_DAYS value: {backfill_days}")
        
        asyncio.run(run_backfill())
    else:
        # Start FastAPI server with collection service
        async def run_server():
            config = uvicorn.Config(
                "enhanced_onchain_collector_v2:create_app",
                host="0.0.0.0",
                port=8004,
                factory=True,
                log_level="info"
            )
            server = uvicorn.Server(config)
            
            # Start collection service in background
            collection_task = asyncio.create_task(collector.run_service())
            
            # Start server
            await server.serve()
        
        asyncio.run(run_server())